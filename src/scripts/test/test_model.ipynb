{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Load and test a local Hugging Face model in GPT-2 format.\n",
    "Supports basic text generation and BLIMP linguistic testing.\n",
    "\"\"\"\n",
    "\n",
    "import typing as t\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "\n",
    "def load_model(\n",
    "    model_path: str,\n",
    "    use_cuda: bool = torch.cuda.is_available(),\n",
    ") -> tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"\n",
    "    Load a local language model and tokenizer from the specified path.\n",
    "    Works with GPT-2 and other causal language models.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the local model directory\n",
    "        use_cuda: Whether to use CUDA for model inference\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (model, tokenizer)\n",
    "    \"\"\"\n",
    "    model_path = Path(model_path)\n",
    "    \n",
    "    # Check if the path exists\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Model path {model_path} does not exist\")\n",
    "    \n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    \n",
    "    # Move model to GPU if available and requested\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"Model loaded successfully to {device}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def generate_text(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    prompt: str,\n",
    "    max_length: int = 100,\n",
    "    num_return_sequences: int = 1,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: int = 50,\n",
    "    repetition_penalty: float = 1.0,\n",
    "    seed: int = 42,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Generate text using the loaded model.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded GPT-2 model\n",
    "        tokenizer: Loaded GPT-2 tokenizer\n",
    "        prompt: Input text to continue from\n",
    "        max_length: Maximum length of generated sequence\n",
    "        num_return_sequences: Number of sequences to generate\n",
    "        temperature: Sampling temperature\n",
    "        top_p: Nucleus sampling parameter\n",
    "        top_k: Top-k sampling parameter\n",
    "        repetition_penalty: Penalty for repeating tokens\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        List of generated text sequences\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Create a text generation pipeline\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "    \n",
    "    # Generate text\n",
    "    outputs = generator(\n",
    "        prompt,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        return_full_text=True,\n",
    "    )\n",
    "    \n",
    "    # Extract generated texts\n",
    "    generated_texts = [output[\"generated_text\"] for output in outputs]\n",
    "    return generated_texts\n",
    "\n",
    "\n",
    "def evaluate_blimp(\n",
    "    model: AutoModelForCausalLM, \n",
    "    tokenizer: AutoTokenizer,\n",
    "    blimp_path: str,\n",
    "    num_samples: int = 100,\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate model on BLiMP (Benchmark of Linguistic Minimal Pairs) tests.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded GPT-2 model\n",
    "        tokenizer: Loaded GPT-2 tokenizer\n",
    "        blimp_path: Path to BLiMP dataset directory or specific test file\n",
    "        num_samples: Number of samples to evaluate (max)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with test name and accuracy\n",
    "    \"\"\"\n",
    "    blimp_path = Path(blimp_path)\n",
    "    results = {}\n",
    "    \n",
    "    # Process single file or directory\n",
    "    if blimp_path.is_file():\n",
    "        files = [blimp_path]\n",
    "    else:\n",
    "        files = list(blimp_path.glob(\"*.jsonl\"))\n",
    "    \n",
    "    for file_path in files:\n",
    "        test_name = file_path.stem\n",
    "        print(f\"Evaluating {test_name}...\")\n",
    "        \n",
    "        # Load BLiMP data\n",
    "        samples = []\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= num_samples:\n",
    "                    break\n",
    "                samples.append(json.loads(line))\n",
    "        \n",
    "        correct = 0\n",
    "        total = len(samples)\n",
    "        \n",
    "        # Process each minimal pair\n",
    "        for sample in samples:\n",
    "            good_sentence = sample[\"sentence_good\"]\n",
    "            bad_sentence = sample[\"sentence_bad\"]\n",
    "            \n",
    "            # Calculate perplexity for both sentences\n",
    "            good_perplexity = calculate_perplexity(model, tokenizer, good_sentence)\n",
    "            bad_perplexity = calculate_perplexity(model, tokenizer, bad_sentence)\n",
    "            \n",
    "            # A linguistically aware model should assign lower perplexity to good sentences\n",
    "            if good_perplexity < bad_perplexity:\n",
    "                correct += 1\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        results[test_name] = accuracy\n",
    "        print(f\"{test_name}: {accuracy:.2%} ({correct}/{total})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_perplexity(\n",
    "    model: AutoModelForCausalLM, \n",
    "    tokenizer: AutoTokenizer, \n",
    "    text: str\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of text using the model.\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded GPT-2 model\n",
    "        tokenizer: Loaded GPT-2 tokenizer\n",
    "        text: Input text\n",
    "        \n",
    "    Returns:\n",
    "        Perplexity score (lower is better)\n",
    "    \"\"\"\n",
    "    # Encode the text\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids = encodings.input_ids.to(model.device)\n",
    "    \n",
    "    # Get model output with cross entropy loss\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        \n",
    "    # Calculate perplexity from cross entropy loss\n",
    "    loss = outputs.loss.item()\n",
    "    perplexity = torch.exp(torch.tensor(loss)).item()\n",
    "    \n",
    "    return perplexity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/jliu/Desktop/checkpoints/1e6_finetuned/1e6_seed_1_entropy_001_lm_loss_001/best_reward were not used when initializing GPT2LMHeadModel: ['v_head.summary.bias', 'v_head.summary.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully to cpu\n"
     ]
    }
   ],
   "source": [
    "# laod model\n",
    "model_path = \"/Users/jliu/Desktop/checkpoints/1e6_finetuned/1e6_seed_1_entropy_001_lm_loss_001/best_reward\"\n",
    "\n",
    "model, tokenizer = load_model(model_path,use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i am  i going to put it in there?']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"i am \"\n",
    "generate_text(model,tokenizer,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.053261756896973"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'i am  i going to put it in there?'\n",
    "calculate_perplexity(model,tokenizer,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Fine-tuning script for decoder-only transformer language models.\n",
    "Loads text files as training data and fine-tunes a pre-trained model.\n",
    "\"\"\"\n",
    "\n",
    "import typing as t\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"Arguments pertaining to which model/config/tokenizer we are going to fine-tune.\"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    tokenizer_name_or_path: str = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Path to pretrained tokenizer or tokenizer identifier (defaults to model_name_or_path)\"}\n",
    "    )\n",
    "    cache_dir: str = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where to store the pretrained models downloaded from huggingface.co\"}\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer implementations\"}\n",
    "    )\n",
    "    torch_dtype: str = field(\n",
    "        default=\"auto\",\n",
    "        metadata={\"help\": \"Floating-point format in which the model weights should be initialized and trained\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    \"\"\"Arguments pertaining to what data we are going to input our model for training and evaluation.\"\"\"\n",
    "\n",
    "    train_file: str = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Path to training file (.txt)\"}\n",
    "    )\n",
    "    train_dir: str = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Directory containing training files (.txt)\"}\n",
    "    )\n",
    "    validation_file: str = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Path to validation file (.txt)\"}\n",
    "    )\n",
    "    validation_dir: str = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Directory containing validation files (.txt)\"}\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=1024,\n",
    "        metadata={\"help\": \"Maximum sequence length that the model might handle\"}\n",
    "    )\n",
    "    preprocessing_num_workers: int = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Number of processes for preprocessing\"}\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    preprocessing_only: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Only run the preprocessing script to be cached for future use\"}\n",
    "    )\n",
    "    keep_linebreaks: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to keep line breaks when processing input files\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    \"\"\"Custom training arguments.\"\"\"\n",
    "    \n",
    "    report_to: str = field(\n",
    "        default=\"tensorboard\",\n",
    "        metadata={\"help\": \"The integration to report the results and logs to.\"}\n",
    "    )\n",
    "    gradient_checkpointing: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Enable gradient checkpointing to save memory at the expense of slower backward pass.\"}\n",
    "    )\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset for loading text files for causal language modeling.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        file_paths: list[str | Path],\n",
    "        block_size: int,\n",
    "        keep_linebreaks: bool = True,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.file_paths = [Path(path) for path in file_paths]\n",
    "        self.block_size = block_size\n",
    "        self.keep_linebreaks = keep_linebreaks\n",
    "        \n",
    "        # Load and tokenize all the texts\n",
    "        logger.info(f\"Loading and tokenizing {len(file_paths)} text files...\")\n",
    "        self.examples = self._load_and_tokenize()\n",
    "        logger.info(f\"Created {len(self.examples)} training examples of size {block_size}\")\n",
    "    \n",
    "    def _load_and_tokenize(self) -> list[torch.Tensor]:\n",
    "        \"\"\"Load all text files and tokenize them into block-sized chunks.\"\"\"\n",
    "        tokenized_examples = []\n",
    "        \n",
    "        for file_path in self.file_paths:\n",
    "            try:\n",
    "                with open(file_path, encoding=\"utf-8\") as f:\n",
    "                    text = f.read()\n",
    "                    \n",
    "                # Optional processing\n",
    "                if not self.keep_linebreaks:\n",
    "                    text = text.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "                \n",
    "                # Tokenize the text\n",
    "                tokenized_text = self.tokenizer(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "                input_ids = tokenized_text.input_ids[0]\n",
    "                \n",
    "                # Create examples of block_size\n",
    "                for i in range(0, len(input_ids) - self.block_size + 1, self.block_size):\n",
    "                    tokenized_examples.append(input_ids[i:i + self.block_size])\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing file {file_path}: {e}\")\n",
    "        \n",
    "        return tokenized_examples\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        return {\"input_ids\": self.examples[idx], \"labels\": self.examples[idx]}\n",
    "\n",
    "\n",
    "def get_file_paths(path: str | Path) -> list[Path]:\n",
    "    \"\"\"Get all .txt file paths from a file or directory.\"\"\"\n",
    "    path = Path(path)\n",
    "    \n",
    "    if path.is_file():\n",
    "        if path.suffix.lower() == \".txt\":\n",
    "            return [path]\n",
    "        else:\n",
    "            logger.warning(f\"Ignoring non-txt file: {path}\")\n",
    "            return []\n",
    "    elif path.is_dir():\n",
    "        return list(path.glob(\"**/*.txt\"))\n",
    "    else:\n",
    "        logger.warning(f\"Path does not exist: {path}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def setup_tokenizer(tokenizer: AutoTokenizer) -> AutoTokenizer:\n",
    "    \"\"\"Setup tokenizer for training by ensuring it has padding token, etc.\"\"\"\n",
    "    special_tokens_dict = {}\n",
    "    \n",
    "    # Ensure the tokenizer has a pad token\n",
    "    if tokenizer.pad_token is None:\n",
    "        if tokenizer.eos_token is not None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        else:\n",
    "            special_tokens_dict[\"pad_token\"] = \"[PAD]\"\n",
    "    \n",
    "    # Add special tokens if needed\n",
    "    if special_tokens_dict:\n",
    "        tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def finetune_model(\n",
    "    model_args: ModelArguments,\n",
    "    data_args: DataArguments,\n",
    "    training_args: TrainingArguments,\n",
    ") -> None:\n",
    "    \"\"\"Fine-tune the model with the given arguments.\"\"\"\n",
    "    # Set seed for reproducibility\n",
    "    set_seed(training_args.seed)\n",
    "    \n",
    "    # Load pretrained model and tokenizer\n",
    "    tokenizer_name = model_args.tokenizer_name_or_path or model_args.model_name_or_path\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_fast=model_args.use_fast_tokenizer,\n",
    "    )\n",
    "    tokenizer = setup_tokenizer(tokenizer)\n",
    "    \n",
    "    # Determine the torch dtype\n",
    "    torch_dtype = (\n",
    "        model_args.torch_dtype\n",
    "        if model_args.torch_dtype in [\"auto\", \"float16\", \"float32\"]\n",
    "        else \"auto\"\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    logger.info(f\"Loading pretrained model from {model_args.model_name_or_path}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        torch_dtype=torch_dtype,\n",
    "    )\n",
    "    \n",
    "    # Resize token embeddings if needed\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Collect text files\n",
    "    train_files = []\n",
    "    if data_args.train_file:\n",
    "        train_files.extend(get_file_paths(data_args.train_file))\n",
    "    if data_args.train_dir:\n",
    "        train_files.extend(get_file_paths(data_args.train_dir))\n",
    "    \n",
    "    validation_files = []\n",
    "    if data_args.validation_file:\n",
    "        validation_files.extend(get_file_paths(data_args.validation_file))\n",
    "    if data_args.validation_dir:\n",
    "        validation_files.extend(get_file_paths(data_args.validation_dir))\n",
    "    \n",
    "    if not train_files:\n",
    "        raise ValueError(\"No training files found. Please specify --train_file or --train_dir\")\n",
    "    \n",
    "    logger.info(f\"Found {len(train_files)} training files and {len(validation_files)} validation files\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_paths=train_files,\n",
    "        block_size=data_args.max_seq_length,\n",
    "        keep_linebreaks=data_args.keep_linebreaks,\n",
    "    )\n",
    "    \n",
    "    validation_dataset = None\n",
    "    if validation_files:\n",
    "        validation_dataset = TextDataset(\n",
    "            tokenizer=tokenizer,\n",
    "            file_paths=validation_files,\n",
    "            block_size=data_args.max_seq_length,\n",
    "            keep_linebreaks=data_args.keep_linebreaks,\n",
    "        )\n",
    "    \n",
    "    # Exit if we only want to preprocess the data\n",
    "    if data_args.preprocessing_only:\n",
    "        logger.info(\"Preprocessing completed. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,  # We're doing causal language modeling, not masked\n",
    "    )\n",
    "    \n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Save the model\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    \n",
    "    # Log and save metrics\n",
    "    metrics = train_result.metrics\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    \n",
    "    logger.info(\"Training completed!\")\n",
    "\n",
    "\n",
    "def test_function() -> None:\n",
    "    \"\"\"\n",
    "    Example showing how to use the script.\n",
    "    \"\"\"\n",
    "    # This is a simple test to show how to use the script\n",
    "    # In practice, you would call this from the command line\n",
    "    \n",
    "    # Example command:\n",
    "    # python finetune_lm.py \\\n",
    "    #     --model_name_or_path gpt2 \\\n",
    "    #     --train_dir ./data/train \\\n",
    "    #     --validation_file ./data/valid/test.txt \\\n",
    "    #     --output_dir ./results \\\n",
    "    #     --num_train_epochs 3 \\\n",
    "    #     --per_device_train_batch_size 4 \\\n",
    "    #     --gradient_accumulation_steps 8 \\\n",
    "    #     --save_steps 500 \\\n",
    "    #     --save_total_limit 2\n",
    "    \n",
    "    # For testing in a Python script:\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "    \n",
    "    model_args = ModelArguments(\n",
    "        model_name_or_path=\"gpt2-small\",\n",
    "    )\n",
    "    \n",
    "    data_args = DataArguments(\n",
    "        train_dir=\"./data/train_texts\",\n",
    "        validation_file=\"./data/valid/sample.txt\",\n",
    "        max_seq_length=128,  # Small for testing\n",
    "    )\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        logging_steps=10,\n",
    "    )\n",
    "    \n",
    "    finetune_model(model_args, data_args, training_args)\n",
    "    print(\"Test fine-tuning completed!\")\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Parse arguments and run fine-tuning.\"\"\"\n",
    "    parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    \n",
    "    # Validate arguments\n",
    "    if not data_args.train_file and not data_args.train_dir:\n",
    "        raise ValueError(\"Need either a training file or directory\")\n",
    "    \n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler()],\n",
    "    )\n",
    "    \n",
    "    logger.setLevel(logging.INFO if training_args.local_rank <= 0 else logging.WARN)\n",
    "    logger.info(f\"Training/evaluation parameters: {training_args}\")\n",
    "    \n",
    "    # Fine-tune model\n",
    "    finetune_model(model_args, data_args, training_args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
