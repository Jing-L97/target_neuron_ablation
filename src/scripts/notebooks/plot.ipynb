{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "示例数据树结构：/Users/curiostudio/Desktop/geometry\n",
    "├── group_best\n",
    "│   ├── activation\n",
    "│   │   └── longtail_50\n",
    "│   │       ├── EleutherAI\n",
    "│   │       │   ├── pythia-410m-deduped\n",
    "│   │       │   │   └── prob\n",
    "│   │       │   │       ├── 500_10.debug\n",
    "│   │       │   │       ├── 500_10.json\n",
    "│   │       │   │       ├── 500_100.json\n",
    "│   │       │   │       └── 500_50.json\n",
    "│   │       │   └── pythia-70m-deduped\n",
    "│   │       │       └── prob\n",
    "│   │       │           ├── 500_10.debug\n",
    "│   │       │           ├── 500_10.json\n",
    "│   │       │           ├── 500_100.json\n",
    "│   │       │           └── 500_50.json\n",
    "│   │       └── prob\n",
    "│   └── weight\n",
    "...\n",
    "├── group_target_size\n",
    "│   ├── activation\n",
    "│   │   └── longtail_50\n",
    "...\n",
    "└── individual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ROOT_dir = Path('/Users/jliu/workspace/RAG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class JsonProcessor:\n",
    "    \"\"\"Class for handling JSON serialization with NumPy type conversion.\"\"\"\n",
    "    @staticmethod\n",
    "    def load_json(filepath: Path) -> dict:\n",
    "        \"\"\"Load a JSON file into a dictionary.\"\"\"\n",
    "        with open(filepath, encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "\n",
    "def slugify(text):\n",
    "    return re.sub(r'\\W+','_', text).strip('_').lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric analyses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepath_list(DATA_DIR:Path,space_type:str,model_name_lst,sample_size_lst,group_type_lst,filename)->list:\n",
    "    \"\"\"Iterate file names.\"\"\"\n",
    "    filepath_lst = []\n",
    "    file_info = []\n",
    "    for group_type in group_type_lst: \n",
    "        for model_name in model_name_lst:\n",
    "            for sample_size in sample_size_lst:\n",
    "                filepath_lst.append(DATA_DIR / group_type / space_type / \"longtail_50/EleutherAI\"/model_name / \"prob\"/ f\"500_{sample_size}.json\")\n",
    "                file_info.append([model_name,sample_size])\n",
    "    return filepath_lst, file_info\n",
    "\n",
    "\n",
    "def load_data(filepath, file_info, step):\n",
    "    \"\"\"Extract stat form json file.\"\"\"\n",
    "    try:\n",
    "        data = json.loads(filepath.read_text(encoding='utf-8'))\n",
    "        records = []\n",
    "        records.append({\n",
    "            'model': file_info[0],\n",
    "            'step': file_info[1],\n",
    "            'sample_size': file_info[2],\n",
    "            'special': data[\"1\"].get('delta_loss_diff'),\n",
    "            'common': data[\"-1\"].get('delta_loss_diff'),\n",
    "        })\n",
    "        return pd.DataFrame.from_records(records)\n",
    "    except:\n",
    "        print(filepath)\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def extract_step_dim(data_all:dict,step:str, file_info:list):\n",
    "    \"\"\"Extract stepwise stat form json file.\"\"\"\n",
    "    try:\n",
    "        data = data_all[step][\"dimensionality\"]\n",
    "        records = []\n",
    "        records.append({\n",
    "            'model': file_info[0],\n",
    "            'step': step,\n",
    "            'sample_size': file_info[2],\n",
    "            'boost': data['boost'].get('dim_prop'),\n",
    "            'suppress': data['suppress'].get('dim_prop'),\n",
    "            'random': data['random_1'].get('dim_prop'),\n",
    "        })\n",
    "        return pd.DataFrame.from_records(records)\n",
    "    except:\n",
    "        print(file_info)\n",
    "        return None\n",
    "\n",
    "\n",
    "# plot the results\n",
    "def plot_metrics_by_model(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str,\n",
    "    shot: int,\n",
    "    out_dir: Path,\n",
    "    show: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    For each model in df, plot 'metric' vs categorical step index,\n",
    "    with separate lines for each sample_size.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for model, group in df.groupby('model'):\n",
    "        # Unique sorted steps across this model\n",
    "        steps = sorted(group['step'].unique())\n",
    "        step_to_idx = {s: i for i, s in enumerate(steps)}\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        # Plot each sample size\n",
    "        for sample_size, color in zip(sorted(group['sample_size'].unique()), ['blue', 'green', 'red']):\n",
    "            sub = group[group['sample_size'] == sample_size]\n",
    "            # Map actual step values to category index\n",
    "            x = [step_to_idx[val] for val in sub['step']]\n",
    "            ax.plot(\n",
    "                x,\n",
    "                sub[metric],\n",
    "                marker='o',\n",
    "                color=color,\n",
    "                label=f\"{sample_size} samples\"\n",
    "            )\n",
    "\n",
    "        # Align x-axis labels to categories\n",
    "        ax.set_xticks(list(range(len(steps))))\n",
    "        ax.set_xticklabels(steps, rotation=45)\n",
    "        ax.set_xlabel('Step')\n",
    "        ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "        ax.set_title(f\"{model} | {shot}-Shot | {metric.replace('_',' ').title()}\")\n",
    "        ax.legend(title='Sample Size')\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "        # Save\n",
    "        slug = slugify(model)\n",
    "        fname = out_dir / f\"{slug}_shot{shot}_{metric}.png\"\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(fname, dpi=200)\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close(fig)\n",
    "        print(f\"Saved plot to {fname}\")\n",
    "\n",
    "\n",
    "\n",
    "def run_pipeline(DATA_DIR,metric_lst,out_dir,model_name_lst,step_lst,sample_size_lst,class_num_lst,group_type_lst,filename):\n",
    "    \"\"\"Run the plotting pipeline.\"\"\"\n",
    "    # load all file pathss\n",
    "    filepath_lst, file_info_lst = get_filepath_list(DATA_DIR,model_name_lst,step_lst,sample_size_lst,class_num_lst,group_type_lst,filename)\n",
    "    # load all files\n",
    "    df = pd.DataFrame()\n",
    "    for file_idx, filepath in enumerate(filepath_lst):\n",
    "        df_row = load_data(filepath, file_info_lst[file_idx])\n",
    "        df = pd.concat([df,df_row])\n",
    "\n",
    "    # plot out the results\n",
    "    for metric in metric_lst:\n",
    "        plot_metrics_by_model(\n",
    "            df,\n",
    "            metric=metric,\n",
    "            shot = 2,\n",
    "            out_dir= out_dir,\n",
    "            show = True\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over differento conditions\n",
    "DATA_DIR = ROOT_dir/'results/individual/weight/longtail/EleutherAI'\n",
    "out_dir= ROOT_dir/'fig/geometry/weight'\n",
    "model_name_lst = [\"pythia-70m-deduped\",\"pythia-410m-deduped\"]\n",
    "step_lst = [0,4000,14000,24000,34000,44000,54000,64000,74000,84000,94000,104000,114000,124000,143000]\n",
    "sample_size_lst = [10, 50, 100]\n",
    "class_num_lst = [2]\n",
    "group_type_lst = [\"individual\"]\n",
    "filename = \"reflection_stat.json\"\n",
    "metric_lst = [\"special\",\"common\"]\n",
    "\n",
    "run_pipeline (DATA_DIR,metric_lst,out_dir,model_name_lst,step_lst,sample_size_lst,class_num_lst,group_type_lst,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepath_list(DATA_DIR,model_name_lst,step_lst,sample_size_lst,class_num_lst,group_type_lst,filename)->list:\n",
    "    \"\"\"Iterate file names.\"\"\"\n",
    "    filepath_lst = []\n",
    "    file_info = []\n",
    "    for model_name in model_name_lst:\n",
    "        for step in step_lst:\n",
    "            for sample_size in sample_size_lst:\n",
    "                for class_num in class_num_lst:\n",
    "                    for group_type in group_type_lst: \n",
    "                        filepath_lst.append(DATA_DIR / model_name / \"prob\" / str(step) / \"500\" / str(sample_size) / str(class_num) / group_type / filename)\n",
    "                        file_info.append([model_name,step,sample_size])\n",
    "    return filepath_lst, file_info\n",
    "\n",
    "\n",
    "def load_data(filepath, file_info):\n",
    "    \"\"\"Extract stat form json file.\"\"\"\n",
    "    try:\n",
    "        data = json.loads(filepath.read_text(encoding='utf-8'))\n",
    "        records = []\n",
    "        records.append({\n",
    "            'model': file_info[0],\n",
    "            'step': file_info[1],\n",
    "            'sample_size': file_info[2],\n",
    "            'special': data[\"1\"].get('delta_loss_diff'),\n",
    "            'common': data[\"-1\"].get('delta_loss_diff'),\n",
    "        })\n",
    "        return pd.DataFrame.from_records(records)\n",
    "    except:\n",
    "        print(filepath)\n",
    "        return None\n",
    "    \n",
    "\n",
    "# plot the results\n",
    "def plot_metrics_by_model(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str,\n",
    "    shot: int,\n",
    "    out_dir: Path,\n",
    "    show: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    For each model in df, plot 'metric' vs categorical step index,\n",
    "    with separate lines for each sample_size.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for model, group in df.groupby('model'):\n",
    "        # Unique sorted steps across this model\n",
    "        steps = sorted(group['step'].unique())\n",
    "        step_to_idx = {s: i for i, s in enumerate(steps)}\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        # Plot each sample size\n",
    "        for sample_size, color in zip(sorted(group['sample_size'].unique()), ['blue', 'green', 'red']):\n",
    "            sub = group[group['sample_size'] == sample_size]\n",
    "            # Map actual step values to category index\n",
    "            x = [step_to_idx[val] for val in sub['step']]\n",
    "            ax.plot(\n",
    "                x,\n",
    "                sub[metric],\n",
    "                marker='o',\n",
    "                color=color,\n",
    "                label=f\"{sample_size} samples\"\n",
    "            )\n",
    "\n",
    "        # Align x-axis labels to categories\n",
    "        ax.set_xticks(list(range(len(steps))))\n",
    "        ax.set_xticklabels(steps, rotation=45)\n",
    "        ax.set_xlabel('Step')\n",
    "        ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "        ax.set_title(f\"{model} | {shot}-Shot | {metric.replace('_',' ').title()}\")\n",
    "        ax.legend(title='Sample Size')\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "        # Save\n",
    "        slug = slugify(model)\n",
    "        fname = out_dir / f\"{slug}_{shot}_{metric}.png\"\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(fname, dpi=200)\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close(fig)\n",
    "        print(f\"Saved plot to {fname}\")\n",
    "\n",
    "\n",
    "\n",
    "def run_pipeline(DATA_DIR,metric_lst,out_dir,model_name_lst,step_lst,sample_size_lst,class_num_lst,group_type_lst,filename):\n",
    "    \"\"\"Run the plotting pipeline.\"\"\"\n",
    "    # load all file pathss\n",
    "    filepath_lst, file_info_lst = get_filepath_list(DATA_DIR,model_name_lst,step_lst,sample_size_lst,class_num_lst,group_type_lst,filename)\n",
    "    # load all files\n",
    "    df = pd.DataFrame()\n",
    "    for file_idx, filepath in enumerate(filepath_lst):\n",
    "        df_row = load_data(filepath, file_info_lst[file_idx])\n",
    "        df = pd.concat([df,df_row])\n",
    "\n",
    "    # plot out the results\n",
    "    for metric in metric_lst:\n",
    "        plot_metrics_by_model(\n",
    "            df,\n",
    "            metric=metric,\n",
    "            shot = 2,\n",
    "            out_dir= out_dir,\n",
    "            show = True\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over differento conditions\n",
    "DATA_DIR = ROOT_dir/'results/classify'\n",
    "out_dir= ROOT_dir/'fig/geometry/reflection'\n",
    "model_name_lst = [\"pythia-70m-deduped\",\"pythia-410m-deduped\"]\n",
    "step_lst = [0,4000,14000,24000,34000,44000,54000,64000,74000,84000,94000,104000,114000,124000,143000]\n",
    "sample_size_lst = [10, 50, 100]\n",
    "class_num_lst = [2]\n",
    "group_type_lst = [\"individual\"]\n",
    "metric_lst = [\"special\",\"common\"]\n",
    "\n",
    "classifier = \"svm_linear\"\n",
    "condition = \"extreme\"\n",
    "filename = f\"ref_{classifier}_{condition}.json\"\n",
    "\n",
    "\n",
    "run_pipeline (DATA_DIR,metric_lst,out_dir,model_name_lst,step_lst,sample_size_lst,class_num_lst,group_type_lst,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over differento conditions\n",
    "DATA_DIR = ROOT_dir/'results/classify'\n",
    "out_dir= ROOT_dir/'fig/geometry/reflection'\n",
    "model_name_lst = [\"pythia-70m-deduped\",\"pythia-410m-deduped\"]\n",
    "step_lst = [0,4000,14000,24000,34000,44000,54000,64000,74000,84000,94000,104000,114000,124000,143000]\n",
    "sample_size_lst = [10, 50, 100]\n",
    "class_num_lst = [2]\n",
    "group_type_lst = [\"individual\"]\n",
    "metric_lst = [\"special\",\"common\"]\n",
    "\n",
    "classifier = \"svm_linear\"\n",
    "condition = \"baseline\"\n",
    "filename = f\"ref_{classifier}_{condition}.json\"\n",
    "\n",
    "\n",
    "run_pipeline (DATA_DIR,metric_lst,out_dir,model_name_lst,step_lst,sample_size_lst,class_num_lst,group_type_lst,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over differento conditions\n",
    "DATA_DIR = ROOT_dir/'results/classify'\n",
    "out_dir= ROOT_dir/'fig/geometry/reflection'\n",
    "model_name_lst = [\"pythia-70m-deduped\",\"pythia-410m-deduped\"]\n",
    "step_lst = [0,4000,14000,24000,34000,44000,54000,64000,74000,84000,94000,104000,114000,124000,143000]\n",
    "sample_size_lst = [10, 50, 100]\n",
    "class_num_lst = [2]\n",
    "group_type_lst = [\"individual\"]\n",
    "filename = \"reflection_stat.json\"\n",
    "metric_lst = [\"special\",\"common\"]\n",
    "\n",
    "run_pipeline (DATA_DIR,metric_lst,out_dir,model_name_lst,step_lst,sample_size_lst,class_num_lst,group_type_lst,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperability analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepath_list(DATA_DIR,model_name_lst,step_lst,sample_size_lst,class_num_lst,group_type_lst,filename)->list:\n",
    "    \"\"\"Iterate file names.\"\"\"\n",
    "    filepath_lst = []\n",
    "    file_info = []\n",
    "    for model_name in model_name_lst:\n",
    "        for step in step_lst:\n",
    "            for sample_size in sample_size_lst:\n",
    "                for class_num in class_num_lst:\n",
    "                    for group_type in group_type_lst: \n",
    "                        filepath_lst.append(DATA_DIR / model_name / \"prob\" / str(step) / \"500\" / str(sample_size) / str(class_num) / group_type / filename)\n",
    "                        file_info.append([model_name,step,sample_size])\n",
    "    return filepath_lst, file_info\n",
    "\n",
    "\n",
    "def load_data(filepath, file_info):\n",
    "    \"\"\"Extract stat form json file.\"\"\"\n",
    "    try:\n",
    "        data_all = json.loads(filepath.read_text(encoding='utf-8'))\n",
    "        data = data_all[\"key_metrics\"][\"average_performance\"]\n",
    "        records = []\n",
    "        records.append({\n",
    "            'model': file_info[0],\n",
    "            'step': file_info[1],\n",
    "            'sample_size': file_info[2],\n",
    "            'acc': data.get('accuracy'),\n",
    "            'f1': data.get('f1_score'),\n",
    "        })\n",
    "        return pd.DataFrame.from_records(records)\n",
    "    except:\n",
    "        print(filepath)\n",
    "        return None\n",
    "    \n",
    "\n",
    "# plot the results\n",
    "def plot_metrics_by_model(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str,\n",
    "    shot: int,\n",
    "    out_dir: Path,\n",
    "    show: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    For each model in df, plot 'metric' vs categorical step index,\n",
    "    with separate lines for each sample_size.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for model, group in df.groupby('model'):\n",
    "        # Unique sorted steps across this model\n",
    "        steps = sorted(group['step'].unique())\n",
    "        step_to_idx = {s: i for i, s in enumerate(steps)}\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        # Plot each sample size\n",
    "        for sample_size, color in zip(sorted(group['sample_size'].unique()), ['blue', 'green', 'red']):\n",
    "            sub = group[group['sample_size'] == sample_size]\n",
    "            # Map actual step values to category index\n",
    "            x = [step_to_idx[val] for val in sub['step']]\n",
    "            ax.plot(\n",
    "                x,\n",
    "                sub[metric],\n",
    "                marker='o',\n",
    "                color=color,\n",
    "                label=f\"{sample_size} samples\"\n",
    "            )\n",
    "\n",
    "        # Align x-axis labels to categories\n",
    "        ax.set_xticks(list(range(len(steps))))\n",
    "        ax.set_xticklabels(steps, rotation=45)\n",
    "        ax.set_xlabel('Step')\n",
    "        ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "        ax.set_title(f\"{model} | {shot}-Shot | {metric.replace('_',' ').title()}\")\n",
    "        ax.legend(title='Sample Size')\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "        # Save\n",
    "        slug = slugify(model)\n",
    "        fname = out_dir / f\"{slug}_{shot}_{metric}.png\"\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(fname, dpi=200)\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close(fig)\n",
    "        print(f\"Saved plot to {fname}\")\n",
    "\n",
    "\n",
    "\n",
    "def run_pipeline(DATA_DIR,metric_lst,out_dir,model_name_lst,step_lst,sample_size_lst,class_num_lst,group_type_lst,filename):\n",
    "    \"\"\"Run the plotting pipeline.\"\"\"\n",
    "    # load all file pathss\n",
    "    filepath_lst, file_info_lst = get_filepath_list(DATA_DIR,model_name_lst,step_lst,sample_size_lst,class_num_lst,group_type_lst,filename)\n",
    "    \n",
    "    # load all files\n",
    "    df = pd.DataFrame()\n",
    "    for file_idx, filepath in enumerate(filepath_lst):\n",
    "        df_row = load_data(filepath, file_info_lst[file_idx])\n",
    "        df = pd.concat([df,df_row])\n",
    "\n",
    "    # plot out the results\n",
    "    for metric in metric_lst:\n",
    "        plot_metrics_by_model(\n",
    "            df,\n",
    "            metric=metric,\n",
    "            shot = 2,\n",
    "            out_dir= out_dir,\n",
    "            show = True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over differento conditions\n",
    "DATA_DIR = ROOT_dir/'results/classify'\n",
    "out_dir= ROOT_dir/'fig/geometry/classify'\n",
    "model_name_lst = [\"pythia-70m-deduped\",\"pythia-410m-deduped\"]\n",
    "step_lst = [0,4000,14000,24000,34000,44000,54000,64000,74000,84000,94000,104000,114000,124000,143000]\n",
    "sample_size_lst = [10, 50, 100]\n",
    "class_num_lst = [2]\n",
    "group_type_lst = [\"individual\",\"group\"]\n",
    "filename = \"separation_analysis_extreme.json\"\n",
    "metric_lst = [\"acc\",\"f1\"]\n",
    "\n",
    "run_pipeline (DATA_DIR,metric_lst,out_dir,model_name_lst,step_lst,sample_size_lst,class_num_lst,group_type_lst,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over differento conditions\n",
    "DATA_DIR = ROOT_dir/'results/classify'\n",
    "out_dir= ROOT_dir/'fig/geometry/classify'\n",
    "model_name_lst = [\"pythia-70m-deduped\",\"pythia-410m-deduped\"]\n",
    "step_lst = [0,4000,14000,24000,34000,44000,54000,64000,74000,84000,94000,104000,114000,124000,143000]\n",
    "sample_size_lst = [10, 50, 100]\n",
    "class_num_lst = [2]\n",
    "group_type_lst = [\"individual\",\"group\"]\n",
    "filename = \"separation_analysis_random.json\"\n",
    "metric_lst = [\"acc\",\"f1\"]\n",
    "\n",
    "run_pipeline (DATA_DIR,metric_lst,out_dir,model_name_lst,step_lst,sample_size_lst,class_num_lst,group_type_lst,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over differento conditions\n",
    "DATA_DIR = ROOT_dir/'results/classify'\n",
    "out_dir= ROOT_dir/'fig/geometry/classify'\n",
    "model_name_lst = [\"pythia-70m-deduped\",\"pythia-410m-deduped\"]\n",
    "step_lst = [0,4000,14000,24000,34000,44000,54000,64000,74000,84000,94000,104000,114000,124000,143000]\n",
    "sample_size_lst = [10, 50, 100]\n",
    "class_num_lst = [2]\n",
    "group_type_lst = [\"individual\",\"group\"]\n",
    "filename = \"separation_analysis_baseline.json\"\n",
    "metric_lst = [\"acc\",\"f1\"]\n",
    "\n",
    "run_pipeline (DATA_DIR,metric_lst,out_dir,model_name_lst,step_lst,sample_size_lst,class_num_lst,group_type_lst,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTSR analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "示例文件树结构：/Users/curiostudio/Desktop/llm_final/classify_updated\n",
    "└── EleutherAI\n",
    "    ├── pythia-410m-deduped\n",
    "    │   └── prob\n",
    "    │       ├── 0\n",
    "    │       │   └── 500\n",
    "    │       │       └── 100\n",
    "    │       │           └── 2\n",
    "    │       │               └── individual\n",
    "    │       │                   ├── classification_baseline.json\n",
    "    │       │                   └── separation_analysis_baseline.json\n",
    "    │       ├── 104000\n",
    "    │       │   └── 500\n",
    "    │       │       ├── 10\n",
    "    │       │       │   ├── 2\n",
    "    │       │       │   │   └── individual\n",
    "    │       │       │   │       ├── classification_baseline.json\n",
    "    │       │       │   │       ├── classification.json\n",
    "    │       │       │   │       ├── separation_analysis_baseline.json\n",
    "    │       │       │   │       └── separation_analysis.json\n",
    "    │       │       │   └── 3\n",
    "    │       │       │       └── individual\n",
    "    │       │       │           ├── classification.json\n",
    "    │       │       │           └── separation_analysis.json\n",
    "    │       │       ├── 100\n",
    "\n",
    "！！！注意：这部分代码是专门处理htsr数据的（见for f in root.rglob('htsr/**/prob/*.json'):—），所以如果是混合上面部分的geometry，也可以直接把母文件路径给它自动跳过不相关数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##classify-htsr\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# —— 原有映射，保持不变 —— \n",
    "SETTING_MAP = {\"group_best\":\"gbest\",\"group_target_size\":\"gtsize\",\"individual\":\"indiv\"}\n",
    "MODEL_MAP   = {\"pythia-70m-deduped\":\"70m\",\"pythia-410m-deduped\":\"410m\"}\n",
    "\n",
    "NEURONS    = [10,50,100]           # 对应文件 500_10.json, 500_50.json, 500_100.json\n",
    "COLORS     = ['blue','green','red']\n",
    "CASE_LIST  = [\"boost\",\"suppress\",\"random_1\"]\n",
    "CASE_STYLE = {\n",
    "    \"boost\":    {\"marker\":\"o\",\"linestyle\":\"-\"},\n",
    "    \"suppress\": {\"marker\":\"s\",\"linestyle\":\"--\"},\n",
    "    \"random_1\": {\"marker\":\"^\",\"linestyle\":\":\"},\n",
    "}\n",
    "\n",
    "# —— 我们的新指标配置 —— \n",
    "STAT_CONFIGS = [\n",
    "    {\"name\":\"ESD Spike Separation\",\"section\":\"esd_shape\",\"subsection\":None,\n",
    "     \"key\":\"spike_separation\",\"yrange\":None},\n",
    "    {\"name\":\"PL Alpha Hill (↓ heavier tail)\",\"section\":\"shape_metrics\",\"subsection\":None,\n",
    "     \"key\":\"pl_alpha_hill\",\"yrange\":None},\n",
    "    {\"name\":\"Phase Structured Indicator\",\"section\":\"phase_transition\",\"subsection\":None,\n",
    "     \"key\":\"phase_metric\",\"yrange\":(-2.1,0.5)},\n",
    "    {\"name\":\"Bulk–Spike Interaction Strength\",\"section\":\"bulk_spike_interaction\",\"subsection\":None,\n",
    "     \"key\":\"interaction_strength\",\"yrange\":None},\n",
    "]\n",
    "\n",
    "CMP_CONFIGS = [\n",
    "    {\"name\":\"Alpha Hill: Boost vs Common\",\"key\":\"alpha_hill_boost_vs_common\"},\n",
    "    {\"name\":\"Alpha Hill: Suppress vs Common\",\"key\":\"alpha_hill_suppress_vs_common\"},\n",
    "    {\"name\":\"Alpha Hill: Suppress vs Boost\",\"key\":\"alpha_hill_suppress_vs_boost\"},\n",
    "    {\"name\":\"Stable Rank: Boost vs Common\",\"key\":\"stable_rank_boost_vs_common\"},\n",
    "    {\"name\":\"Stable Rank: Suppress vs Common\",\"key\":\"stable_rank_suppress_vs_common\"},\n",
    "    {\"name\":\"Stable Rank: Suppress vs Boost\",\"key\":\"stable_rank_suppress_vs_boost\"},\n",
    "    {\"name\":\"Entropy: Boost vs Common\",\"key\":\"entropy_boost_vs_common\"},\n",
    "    {\"name\":\"Entropy: Suppress vs Common\",\"key\":\"entropy_suppress_vs_common\"},\n",
    "    {\"name\":\"Entropy: Suppress vs Boost\",\"key\":\"entropy_suppress_vs_boost\"},\n",
    "]\n",
    "\n",
    "def slugify(text):\n",
    "    return re.sub(r'\\W+','_', text).strip('_').lower()\n",
    "\n",
    "def load_all_json(root_dir):\n",
    "    root = Path(root_dir)\n",
    "    recs = []\n",
    "    for f in root.rglob('htsr/**/prob/*.json'):\n",
    "        parts = list(f.relative_to(root).parts)\n",
    "        # 去掉末尾的 'prob' 与文件名\n",
    "        dir_parts = parts[:-2]\n",
    "        recs.append({\n",
    "            'dir_parts': dir_parts,\n",
    "            'filename': f.name,\n",
    "            'data': json.loads(f.read_text(encoding='utf-8'))\n",
    "        })\n",
    "    return recs\n",
    "\n",
    "def plot_combined(root_dir, out_dir):\n",
    "    recs = load_all_json(root_dir)\n",
    "\n",
    "    # 按所在文件夹分组\n",
    "    groups = {}\n",
    "    for r in recs:\n",
    "        key = tuple(r['dir_parts'])\n",
    "        groups.setdefault(key, []).append(r)\n",
    "\n",
    "    out = Path(out_dir); out.mkdir(parents=True, exist_ok=True)\n",
    "    ncols = 2\n",
    "    nrows = (len(STAT_CONFIGS)+1)//2\n",
    "\n",
    "    for dir_parts, files in groups.items():\n",
    "        # 生成图标题\n",
    "        title_parts = [SETTING_MAP.get(p, MODEL_MAP.get(p, p)) for p in dir_parts]\n",
    "        suptitle = \" | \".join(title_parts)\n",
    "\n",
    "        # 收集不同 size（10/50/100）对应的数据\n",
    "        data_dict = {}\n",
    "        for r in files:\n",
    "            m = re.match(r'500_(\\d+)\\.json', r['filename'])\n",
    "            if m:\n",
    "                size = int(m.group(1))\n",
    "                data_dict[size] = r['data']\n",
    "        if not data_dict:\n",
    "            continue\n",
    "\n",
    "        # —— 第一张：四个指标折线图 —— \n",
    "        fig, axes = plt.subplots(nrows, ncols,\n",
    "                                 figsize=(ncols*6, nrows*4),\n",
    "                                 squeeze=False)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for ax, stat in zip(axes, STAT_CONFIGS):\n",
    "            any_curve = False\n",
    "\n",
    "            for i, nn in enumerate(NEURONS):\n",
    "                data = data_dict.get(nn)\n",
    "                if data is None:\n",
    "                    continue\n",
    "\n",
    "                # x 轴：样本 ID，为了可视化我们用排序后的整数 ID\n",
    "                steps = sorted(int(k) for k in data.keys())\n",
    "                x = steps\n",
    "\n",
    "                for case in CASE_LIST:\n",
    "                    y = []\n",
    "                    for s in steps:\n",
    "                        entry = data[str(s)].get(case, {})\n",
    "                        # 对 phase 字段单独处理\n",
    "                        if stat[\"key\"] == \"phase\":\n",
    "                            phase = entry.get(\"phase_transition\",{}).get(\"phase\")\n",
    "                            v = 1.0 if phase == \"structured\" else 0.0\n",
    "                        else:\n",
    "                            v = entry.get(stat[\"section\"],{}).get(stat[\"key\"])\n",
    "                        y.append(np.nan if v is None else v)\n",
    "\n",
    "                    if all(np.isnan(y)):\n",
    "                        continue\n",
    "\n",
    "                    style = CASE_STYLE[case]\n",
    "                    ax.plot(x, y,\n",
    "                            color=COLORS[i],\n",
    "                            marker=style[\"marker\"],\n",
    "                            linestyle=style[\"linestyle\"],\n",
    "                            label=f\"{case} {nn}\")\n",
    "                    any_curve = True\n",
    "\n",
    "            if not any_curve:\n",
    "                ax.text(0.5,0.5,\"No data\", ha='center', va='center', alpha=0.3)\n",
    "\n",
    "            ax.set_title(stat[\"name\"])\n",
    "            ax.set_xlabel(\"Sample ID\")\n",
    "            ax.set_ylabel(stat[\"name\"])\n",
    "            \n",
    "            if stat[\"yrange\"]:\n",
    "                ax.set_ylim(*stat[\"yrange\"])\n",
    "            ax.grid(alpha=0.3)\n",
    "            ax.legend(title=\"# Samples / Case\", fontsize='small')\n",
    "\n",
    "        fig.suptitle(suptitle, fontsize=14)\n",
    "        fig.tight_layout(rect=[0,0,1,0.95])\n",
    "        fname1 = out / f\"{slugify(suptitle)}_metrics.png\"\n",
    "        fig.savefig(fname1, dpi=200)\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved {fname1}\")\n",
    "\n",
    "        # —— 第二张：三项跨组比较 —— \n",
    "        fig2, axes2 = plt.subplots(1, len(CMP_CONFIGS),\n",
    "                                   figsize=(len(CMP_CONFIGS)*5, 4),\n",
    "                                   squeeze=False)\n",
    "        axes2 = axes2.flatten()\n",
    "\n",
    "        for ax, cmp_stat in zip(axes2, CMP_CONFIGS):\n",
    "            any_curve = False\n",
    "\n",
    "            for i, nn in enumerate(NEURONS):\n",
    "                data = data_dict.get(nn)\n",
    "                if data is None:\n",
    "                    continue\n",
    "\n",
    "                steps = sorted(int(k) for k in data.keys())\n",
    "                x = steps\n",
    "                y = []\n",
    "                for s in steps:\n",
    "                    comp = data[str(s)].get(\"comparisons\", {})\n",
    "                    # 根据 key 前缀选择所属子模块\n",
    "                    if cmp_stat[\"key\"].startswith(\"alpha_hill\"):\n",
    "                        v = comp.get(\"shape_metrics\",{}).get(cmp_stat[\"key\"])\n",
    "                    elif cmp_stat[\"key\"].startswith(\"spike\"):\n",
    "                        v = comp.get(\"esd_shape\",{}).get(cmp_stat[\"key\"])\n",
    "                    else:\n",
    "                        v = comp.get(\"bulk_spike_interaction\",{}).get(cmp_stat[\"key\"])\n",
    "                    y.append(np.nan if v is None else v)\n",
    "\n",
    "                if all(np.isnan(y)):\n",
    "                    continue\n",
    "\n",
    "                # 直接重用 CASE_STYLE 里的 marker/linestyle，也可以定义新的 CMP_STYLE\n",
    "                style = CASE_STYLE.get(cmp_stat[\"key\"].split(\"_\")[0], {\"marker\":\"o\",\"linestyle\":\"-\"})\n",
    "                ax.plot(x, y,\n",
    "                        color=COLORS[i],\n",
    "                        marker=style[\"marker\"],\n",
    "                        linestyle=style[\"linestyle\"],\n",
    "                        label=str(nn))\n",
    "                any_curve = True\n",
    "\n",
    "            if not any_curve:\n",
    "                ax.text(0.5,0.5,\"No data\", ha='center', va='center', alpha=0.3)\n",
    "\n",
    "            ax.set_title(cmp_stat[\"name\"])\n",
    "            ax.set_xlabel(\"Sample ID\")\n",
    "            ax.set_ylabel(\"Δ Value\")\n",
    "            ax.grid(alpha=0.3)\n",
    "            ax.legend(title=\"# Samples\", fontsize='small')\n",
    "\n",
    "        fig2.suptitle(suptitle + \" — Comparisons\", fontsize=14)\n",
    "        fig2.tight_layout(rect=[0,0,1,0.93])\n",
    "        fname2 = out / f\"{slugify(suptitle)}_comparisons.png\"\n",
    "        fig2.savefig(fname2, dpi=200)\n",
    "        plt.close(fig2)\n",
    "        print(f\"Saved {fname2}\")\n",
    "\n",
    "# —— 示例调用 —— \n",
    "if __name__ == '__main__':\n",
    "    plot_combined(\n",
    "        root_dir='/Users/jliu/workspace/RAG/results/geometry',\n",
    "        out_dir='/Users/jliu/workspace/RAG/fig/geometry/htsr'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# First, update the STAT_CONFIGS to include explicit y-axis ranges for all metrics\n",
    "STAT_CONFIGS = [\n",
    "    {\"name\":\"ESD Spike Separation\", \"section\":\"esd_shape\", \"subsection\":None,\n",
    "     \"key\":\"spike_separation\", \"yrange\":(0, 5)},  # Added fixed range\n",
    "    {\"name\":\"PL Alpha Hill (↓ heavier tail)\", \"section\":\"shape_metrics\", \"subsection\":None,\n",
    "     \"key\":\"pl_alpha_hill\", \"yrange\":(0, 20)},  # Added fixed range\n",
    "    {\"name\":\"Phase Structured Indicator\", \"section\":\"phase_transition\", \"subsection\":None,\n",
    "     \"key\":\"phase_metric\", \"yrange\":(-2.1, 0.5)},  # Already had range\n",
    "    {\"name\":\"Bulk–Spike Interaction Strength\", \"section\":\"bulk_spike_interaction\", \"subsection\":None,\n",
    "     \"key\":\"interaction_strength\", \"yrange\":(0, 1.5)},  # Added fixed range\n",
    "]\n",
    "\n",
    "# Also add y-axis ranges for comparison plots\n",
    "CMP_CONFIGS = [\n",
    "    {\"name\":\"Alpha Hill: Boost vs Common\", \"key\":\"alpha_hill_boost_vs_common\", \"yrange\":(0, 20)},\n",
    "    {\"name\":\"Alpha Hill: Suppress vs Common\", \"key\":\"alpha_hill_suppress_vs_common\", \"yrange\":(0, 20)},\n",
    "    {\"name\":\"Alpha Hill: Suppress vs Boost\", \"key\":\"alpha_hill_suppress_vs_boost\", \"yrange\":(0, 20)},\n",
    "    {\"name\":\"Stable Rank: Boost vs Common\", \"key\":\"stable_rank_boost_vs_common\", \"yrange\":(-50, 50)},\n",
    "    {\"name\":\"Stable Rank: Suppress vs Common\", \"key\":\"stable_rank_suppress_vs_common\", \"yrange\":(-50, 50)},\n",
    "    {\"name\":\"Stable Rank: Suppress vs Boost\", \"key\":\"stable_rank_suppress_vs_boost\", \"yrange\":(-50, 50)},\n",
    "    {\"name\":\"Entropy: Boost vs Common\", \"key\":\"entropy_boost_vs_common\", \"yrange\":(-0.5, 0.5)},\n",
    "    {\"name\":\"Entropy: Suppress vs Common\", \"key\":\"entropy_suppress_vs_common\", \"yrange\":(-0.5, 0.5)},\n",
    "    {\"name\":\"Entropy: Suppress vs Boost\", \"key\":\"entropy_suppress_vs_boost\", \"yrange\":(-0.5, 0.5)},\n",
    "]\n",
    "\n",
    "def plot_combined(root_dir: str, out_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot combined metrics and comparisons with fixed y-axis ranges.\n",
    "    \n",
    "    Args:\n",
    "        root_dir: Root directory containing the data files\n",
    "        out_dir: Output directory for saving plots\n",
    "    \"\"\"\n",
    "    recs = load_all_json(root_dir)\n",
    "\n",
    "    # Group by directory\n",
    "    groups: dict[tuple[str, ...], list[dict]] = {}\n",
    "    for r in recs:\n",
    "        key = tuple(r['dir_parts'])\n",
    "        groups.setdefault(key, []).append(r)\n",
    "\n",
    "    out = Path(out_dir)\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "    ncols = 2\n",
    "    nrows = (len(STAT_CONFIGS) + 1) // 2\n",
    "\n",
    "    for dir_parts, files in groups.items():\n",
    "        # Generate title\n",
    "        title_parts = [SETTING_MAP.get(p, MODEL_MAP.get(p, p)) for p in dir_parts]\n",
    "        suptitle = \" | \".join(title_parts)\n",
    "\n",
    "        # Collect data for different sizes (10/50/100)\n",
    "        data_dict: dict[int, dict] = {}\n",
    "        for r in files:\n",
    "            m = re.match(r'500_(\\d+)\\.json', r['filename'])\n",
    "            if m:\n",
    "                size = int(m.group(1))\n",
    "                data_dict[size] = r['data']\n",
    "        if not data_dict:\n",
    "            continue\n",
    "\n",
    "        # First figure: four metrics line plots\n",
    "        fig, axes = plt.subplots(nrows, ncols,\n",
    "                                figsize=(ncols*6, nrows*4),\n",
    "                                squeeze=False)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for ax, stat in zip(axes, STAT_CONFIGS):\n",
    "            any_curve = False\n",
    "\n",
    "            for i, nn in enumerate(NEURONS):\n",
    "                data = data_dict.get(nn)\n",
    "                if data is None:\n",
    "                    continue\n",
    "\n",
    "                # x-axis: sample ID\n",
    "                steps = sorted(int(k) for k in data.keys())\n",
    "                x = steps\n",
    "\n",
    "                for case in CASE_LIST:\n",
    "                    y = []\n",
    "                    for s in steps:\n",
    "                        entry = data[str(s)].get(case, {})\n",
    "                        # Special handling for phase field\n",
    "                        if stat[\"key\"] == \"phase\":\n",
    "                            phase = entry.get(\"phase_transition\", {}).get(\"phase\")\n",
    "                            v = 1.0 if phase == \"structured\" else 0.0\n",
    "                        else:\n",
    "                            section_data = entry.get(stat[\"section\"], {})\n",
    "                            v = section_data.get(stat[\"key\"]) if section_data else None\n",
    "                        y.append(np.nan if v is None else v)\n",
    "\n",
    "                    if all(np.isnan(y)):\n",
    "                        continue\n",
    "\n",
    "                    style = CASE_STYLE[case]\n",
    "                    ax.plot(x, y,\n",
    "                           color=COLORS[i],\n",
    "                           marker=style[\"marker\"],\n",
    "                           linestyle=style[\"linestyle\"],\n",
    "                           label=f\"{case} {nn}\")\n",
    "                    any_curve = True\n",
    "\n",
    "            if not any_curve:\n",
    "                ax.text(0.5, 0.5, \"No data\", ha='center', va='center', alpha=0.3)\n",
    "\n",
    "            ax.set_title(stat[\"name\"])\n",
    "            ax.set_xlabel(\"Sample ID\")\n",
    "            ax.set_ylabel(stat[\"name\"])\n",
    "            \n",
    "            # Always set y-axis range if provided\n",
    "            if stat[\"yrange\"] is not None:\n",
    "                ax.set_ylim(*stat[\"yrange\"])\n",
    "            ax.grid(alpha=0.3)\n",
    "            ax.legend(title=\"# Samples / Case\", fontsize='small')\n",
    "\n",
    "        fig.suptitle(suptitle, fontsize=14)\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        fname1 = out / f\"{slugify(suptitle)}_metrics.png\"\n",
    "        fig.savefig(fname1, dpi=200)\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved {fname1}\")\n",
    "\n",
    "        # Second figure: comparisons\n",
    "        fig2, axes2 = plt.subplots(1, len(CMP_CONFIGS),\n",
    "                                 figsize=(len(CMP_CONFIGS)*5, 4),\n",
    "                                 squeeze=False)\n",
    "        axes2 = axes2.flatten()\n",
    "\n",
    "        for ax, cmp_stat in zip(axes2, CMP_CONFIGS):\n",
    "            any_curve = False\n",
    "\n",
    "            for i, nn in enumerate(NEURONS):\n",
    "                data = data_dict.get(nn)\n",
    "                if data is None:\n",
    "                    continue\n",
    "\n",
    "                steps = sorted(int(k) for k in data.keys())\n",
    "                x = steps\n",
    "                y = []\n",
    "                for s in steps:\n",
    "                    comp = data[str(s)].get(\"comparisons\", {})\n",
    "                    # Choose appropriate submodule based on key prefix\n",
    "                    if cmp_stat[\"key\"].startswith(\"alpha_hill\"):\n",
    "                        v = comp.get(\"shape_metrics\", {}).get(cmp_stat[\"key\"])\n",
    "                    elif cmp_stat[\"key\"].startswith(\"spike\"):\n",
    "                        v = comp.get(\"esd_shape\", {}).get(cmp_stat[\"key\"])\n",
    "                    else:\n",
    "                        v = comp.get(\"bulk_spike_interaction\", {}).get(cmp_stat[\"key\"])\n",
    "                    y.append(np.nan if v is None else v)\n",
    "\n",
    "                if all(np.isnan(y)):\n",
    "                    continue\n",
    "\n",
    "                style = CASE_STYLE.get(cmp_stat[\"key\"].split(\"_\")[0], {\"marker\":\"o\", \"linestyle\":\"-\"})\n",
    "                ax.plot(x, y,\n",
    "                       color=COLORS[i],\n",
    "                       marker=style[\"marker\"],\n",
    "                       linestyle=style[\"linestyle\"],\n",
    "                       label=str(nn))\n",
    "                any_curve = True\n",
    "\n",
    "            if not any_curve:\n",
    "                ax.text(0.5, 0.5, \"No data\", ha='center', va='center', alpha=0.3)\n",
    "\n",
    "            ax.set_title(cmp_stat[\"name\"])\n",
    "            ax.set_xlabel(\"Sample ID\")\n",
    "            ax.set_ylabel(\"Δ Value\")\n",
    "            \n",
    "            # Apply fixed y-axis range if provided\n",
    "            if \"yrange\" in cmp_stat and cmp_stat[\"yrange\"] is not None:\n",
    "                ax.set_ylim(*cmp_stat[\"yrange\"])\n",
    "            ax.grid(alpha=0.3)\n",
    "            ax.legend(title=\"# Samples\", fontsize='small')\n",
    "\n",
    "        fig2.suptitle(suptitle + \" — Comparisons\", fontsize=14)\n",
    "        fig2.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "        fname2 = out / f\"{slugify(suptitle)}_comparisons.png\"\n",
    "        fig2.savefig(fname2, dpi=200)\n",
    "        plt.close(fig2)\n",
    "        print(f\"Saved {fname2}\")\n",
    "\n",
    "\n",
    "\n",
    "# —— 示例调用 —— \n",
    "if __name__ == '__main__':\n",
    "    plot_combined(\n",
    "        root_dir='/Users/jliu/workspace/RAG/results/geometry',\n",
    "        out_dir='/Users/jliu/workspace/RAG/fig/geometry/htsr'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "示例文件夹树结构：/Users/curiostudio/Desktop/llm_final/classify_updated\n",
    "└── EleutherAI\n",
    "    ├── pythia-410m-deduped\n",
    "    │   └── prob\n",
    "    │       ├── 0\n",
    "    │       │   └── 500\n",
    "    │       │       └── 100\n",
    "    │       │           └── 2\n",
    "    │       │               └── individual\n",
    "    │       │                   ├── classification_baseline.json\n",
    "    │       │                   └── separation_analysis_baseline.json\n",
    "    │       ├── 104000\n",
    "    │       │   └── 500\n",
    "    │       │       ├── 10\n",
    "    │       │       │   ├── 2\n",
    "    │       │       │   │   └── individual\n",
    "    │       │       │   │       ├── classification_baseline.json\n",
    "    │       │       │   │       ├── classification.json\n",
    "    │       │       │   │       ├── separation_analysis_baseline.json\n",
    "    │       │       │   │       └── separation_analysis.json\n",
    "    │       │       │   └── 3\n",
    "    │       │       │       └── individual\n",
    "    │       │       │           ├── classification.json\n",
    "    │       │       │           └── separation_analysis.json\n",
    "    \n",
    "!!!注意：以下版本是处理带_baseline版本的新数据，如果命名更改的话注意是否带_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation Analysis Notebook\n",
    "\n",
    "## Cell 1: Imports and Settings\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ========== User-Configurable Settings ==========\n",
    "# Root of your 'classify' folder\\\n",
    "ROOT_DIR = Path('/Users/curiostudio/Desktop/llm_final/classify_updated')\\\n",
    "# Choose shot: 2 or 3\\\n",
    "SHOT_CHOICE = 2\\\n",
    "# Sample sizes to keep\\\n",
    "SAMPLE_SIZES = [10, 50, 100, 'unified']\\\n",
    "# Output directory for plots\\\n",
    "OUT_DIR = Path('plots_classify_updated0509')\\\n",
    "# Optional: whether to display plots inline\\\n",
    "SHOW_PLOTS = True\\\n",
    "# ===============================================\n",
    "\n",
    "\n",
    "## Cell 2: Data Collection (Only Individual Config)\n",
    "\n",
    "def slugify(text: str) -> str:\n",
    "    \"\"\"Convert arbitrary text into a filesystem-safe slug.\"\"\"\n",
    "    return re.sub(r'[^A-Za-z0-9]+', '_', text).strip('_').lower()\n",
    "\n",
    "\n",
    "def collect_separation_data(\n",
    "    root_dir: Path,\n",
    "    shot: int,\n",
    "    sample_sizes=tuple(SAMPLE_SIZES)\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Traverse all separation_analysis.json under ROOT_DIR,\n",
    "    but only load those under 'individual' directories,\n",
    "    extract average_performance accuracy & f1_score for the given shot and sample_sizes.\n",
    "    Returns a DataFrame with columns: ['model', 'step', 'sample_size', 'accuracy', 'f1_score']\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for json_path in root_dir.rglob('separation_analysis_baseline.json'):\n",
    "        # Expected structure:\n",
    "        # EleutherAI/<model>/prob/<step>/500/<sample_size>/<shot>/individual/separation_analysis.json\n",
    "        parts = json_path.relative_to(root_dir).parts\n",
    "        if len(parts) < 9 or parts[0] != 'EleutherAI' or parts[2] != 'prob':\n",
    "            continue\n",
    "        # Only include 'individual' config, skip group_best, group_target_size, unified, etc.\n",
    "        if parts[7] != 'individual':\n",
    "            continue\n",
    "\n",
    "        model = parts[1]\n",
    "        try:\n",
    "            step = int(parts[3])\n",
    "            sample_size = int(parts[5])\n",
    "            sh = int(parts[6])\n",
    "        except ValueError:\n",
    "            # Skip non-numeric directories\n",
    "            continue\n",
    "\n",
    "        # Filter by shot and sample size\n",
    "        if sh != shot or sample_size not in sample_sizes:\n",
    "            continue\n",
    "        # Skip non-positive step to avoid log(0)\n",
    "        if step <= 0:\n",
    "            continue\n",
    "\n",
    "        # Load JSON and extract\n",
    "        data = json.loads(json_path.read_text(encoding='utf-8'))\n",
    "        avg = data['key_metrics']['average_performance']\n",
    "        records.append({\n",
    "            'model': model,\n",
    "            'step': step,\n",
    "            'sample_size': sample_size,\n",
    "            'accuracy': avg.get('accuracy'),\n",
    "            'f1_score': avg.get('f1_score'),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    if df.empty:\n",
    "        raise RuntimeError(f\"No data found for {shot}-shot with sizes {sample_sizes} at {root_dir}\")\n",
    "    # Sort by model and step\n",
    "    return df.sort_values(['model','step'])\n",
    "\n",
    "## Cell 3: Plotting with Categorical X-Axis\n",
    "\n",
    "def plot_metrics_by_model(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str,\n",
    "    shot: int,\n",
    "    out_dir: Path,\n",
    "    show: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    For each model in df, plot 'metric' vs categorical step index,\n",
    "    with separate lines for each sample_size.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for model, group in df.groupby('model'):\n",
    "        # Unique sorted steps across this model\n",
    "        steps = sorted(group['step'].unique())\n",
    "        step_to_idx = {s: i for i, s in enumerate(steps)}\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        # Plot each sample size\n",
    "        for sample_size, color in zip(sorted(group['sample_size'].unique()), ['blue', 'green', 'red']):\n",
    "            sub = group[group['sample_size'] == sample_size]\n",
    "            # Map actual step values to category index\n",
    "            x = [step_to_idx[val] for val in sub['step']]\n",
    "            ax.plot(\n",
    "                x,\n",
    "                sub[metric],\n",
    "                marker='o',\n",
    "                color=color,\n",
    "                label=f\"{sample_size} samples\"\n",
    "            )\n",
    "\n",
    "        # Align x-axis labels to categories\n",
    "        ax.set_xticks(list(range(len(steps))))\n",
    "        ax.set_xticklabels(steps, rotation=45)\n",
    "        ax.set_xlabel('Step')\n",
    "        ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "        ax.set_title(f\"{model} | {shot}-Shot | {metric.replace('_',' ').title()}\")\n",
    "        ax.legend(title='Sample Size')\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "        # Save\n",
    "        slug = slugify(model)\n",
    "        fname = out_dir / f\"{slug}_shot{shot}_{metric}.png\"\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(fname, dpi=200)\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close(fig)\n",
    "        print(f\"Saved plot to {fname}\")\n",
    "\n",
    "## Cell 4: Execute Collection & Plotting\n",
    "\n",
    "# 1. Collect the filtered data\n",
    "df_all = collect_separation_data(ROOT_DIR, SHOT_CHOICE)\n",
    "\n",
    "# 2. Plot both metrics\n",
    "for metric in ['accuracy', 'f1_score']:\n",
    "    plot_metrics_by_model(df_all, metric, SHOT_CHOICE, OUT_DIR, show=SHOW_PLOTS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection analyses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepath_list(DATA_DIR,model_name_lst,step_lst,sample_size_lst,class_num_lst,group_type_lst,filename)->list:\n",
    "    \"\"\"Iterate file names.\"\"\"\n",
    "    filepath_lst = []\n",
    "    file_info = []\n",
    "    for model_name in model_name_lst:\n",
    "        for step in step_lst:\n",
    "            for sample_size in sample_size_lst:\n",
    "                for class_num in class_num_lst:\n",
    "                    for group_type in group_type_lst: \n",
    "                        filepath_lst.append(DATA_DIR / model_name / \"prob\" / str(step) / \"500\" / str(sample_size) / str(class_num) / group_type / filename)\n",
    "                        file_info.append([model_name,step,sample_size])\n",
    "    return filepath_lst, file_info\n",
    "\n",
    "\n",
    "def load_data(filepath, file_info):\n",
    "    \"\"\"Extract stat form json file.\"\"\"\n",
    "    try:\n",
    "        data = json.loads(filepath.read_text(encoding='utf-8'))\n",
    "        records = []\n",
    "        records.append({\n",
    "            'model': file_info[0],\n",
    "            'step': file_info[1],\n",
    "            'sample_size': file_info[2],\n",
    "            'special': data[\"1\"].get('delta_loss_diff'),\n",
    "            'common': data[\"-1\"].get('delta_loss_diff'),\n",
    "        })\n",
    "        return pd.DataFrame.from_records(records)\n",
    "    except:\n",
    "        print(filepath)\n",
    "        return None\n",
    "    \n",
    "\n",
    "# plot the results\n",
    "def plot_metrics_by_model(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str,\n",
    "    shot: int,\n",
    "    out_dir: Path,\n",
    "    show: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    For each model in df, plot 'metric' vs categorical step index,\n",
    "    with separate lines for each sample_size.\n",
    "    \"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for model, group in df.groupby('model'):\n",
    "        # Unique sorted steps across this model\n",
    "        steps = sorted(group['step'].unique())\n",
    "        step_to_idx = {s: i for i, s in enumerate(steps)}\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        # Plot each sample size\n",
    "        for sample_size, color in zip(sorted(group['sample_size'].unique()), ['blue', 'green', 'red']):\n",
    "            sub = group[group['sample_size'] == sample_size]\n",
    "            # Map actual step values to category index\n",
    "            x = [step_to_idx[val] for val in sub['step']]\n",
    "            ax.plot(\n",
    "                x,\n",
    "                sub[metric],\n",
    "                marker='o',\n",
    "                color=color,\n",
    "                label=f\"{sample_size} samples\"\n",
    "            )\n",
    "\n",
    "        # Align x-axis labels to categories\n",
    "        ax.set_xticks(list(range(len(steps))))\n",
    "        ax.set_xticklabels(steps, rotation=45)\n",
    "        ax.set_xlabel('Step')\n",
    "        ax.set_ylabel(metric.replace('_', ' ').title())\n",
    "        ax.set_title(f\"{model} | {shot}-Shot | {metric.replace('_',' ').title()}\")\n",
    "        ax.legend(title='Sample Size')\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "        # Save\n",
    "        slug = slugify(model)\n",
    "        fname = out_dir / f\"{slug}_shot{shot}_{metric}.png\"\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(fname, dpi=200)\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close(fig)\n",
    "        print(f\"Saved plot to {fname}\")\n",
    "\n",
    "\n",
    "\n",
    "def run_pipeline(DATA_DIR,metric_lst,out_dir,model_name_lst,step_lst,sample_size_lst,class_num_lst,group_type_lst,filename):\n",
    "    \"\"\"Run the plotting pipeline.\"\"\"\n",
    "    # load all file pathss\n",
    "    filepath_lst, file_info_lst = get_filepath_list(DATA_DIR,model_name_lst,step_lst,sample_size_lst,class_num_lst,group_type_lst,filename)\n",
    "    # load all files\n",
    "    df = pd.DataFrame()\n",
    "    for file_idx, filepath in enumerate(filepath_lst):\n",
    "        df_row = load_data(filepath, file_info_lst[file_idx])\n",
    "        df = pd.concat([df,df_row])\n",
    "\n",
    "    # plot out the results\n",
    "    for metric in metric_lst:\n",
    "        plot_metrics_by_model(\n",
    "            df,\n",
    "            metric=metric,\n",
    "            shot = 2,\n",
    "            out_dir= out_dir,\n",
    "            show = True\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over differento conditions\n",
    "DATA_DIR = Path('/Users/jliu/workspace/RAG/results/classify')\n",
    "out_dir= Path('/Users/jliu/workspace/RAG/fig/geometry/reflection')\n",
    "model_name_lst = [\"pythia-70m-deduped\",\"pythia-410m-deduped\"]\n",
    "step_lst = [0,4000,14000,24000,34000,44000,54000,64000,74000,84000,94000,104000,114000,124000,143000]\n",
    "sample_size_lst = [10, 50, 100]\n",
    "class_num_lst = [2]\n",
    "group_type_lst = [\"individual\"]\n",
    "filename = \"reflection_stat.json\"\n",
    "metric_lst = [\"special\",\"common\"]\n",
    "\n",
    "run_pipeline (DATA_DIR,metric_lst,out_dir,model_name_lst,step_lst,sample_size_lst,class_num_lst,group_type_lst,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## our assumption: \n",
    "special: reduced delta loss after reflection \n",
    "-> negative for the comparison\n",
    "\n",
    "\n",
    "common: increased delta loss \n",
    "-> positive for the comparison "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
