{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdedb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#三色比例图，直接更改底部main函数中的参数\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import ast\n",
    "from scipy import stats\n",
    "import json\n",
    "\n",
    "def calculate_phase_metrics(log_ranks, log_vals, zipf_line, plateau_log_rank, elbow_log_rank):\n",
    "    metrics = {}\n",
    "    \n",
    "    plateau_idx = np.where(log_ranks >= plateau_log_rank)[0][0] if any(log_ranks >= plateau_log_rank) else 0\n",
    "    elbow_idx = np.where(log_ranks >= elbow_log_rank)[0][0] if any(log_ranks >= elbow_log_rank) else len(log_ranks)\n",
    "    \n",
    "    plateau_mask = np.arange(len(log_ranks)) < plateau_idx\n",
    "    linear_mask = (np.arange(len(log_ranks)) >= plateau_idx) & (np.arange(len(log_ranks)) < elbow_idx)\n",
    "    fast_decay_mask = np.arange(len(log_ranks)) >= elbow_idx\n",
    "    \n",
    "    metrics[\"plateau_size\"] = np.sum(plateau_mask)\n",
    "    metrics[\"linear_size\"] = np.sum(linear_mask)\n",
    "    metrics[\"fast_decay_size\"] = np.sum(fast_decay_mask)\n",
    "    \n",
    "    total = len(log_ranks)\n",
    "    metrics[\"plateau_percent\"] = 100 * metrics[\"plateau_size\"] / total\n",
    "    metrics[\"linear_percent\"] = 100 * metrics[\"linear_size\"] / total\n",
    "    metrics[\"fast_decay_percent\"] = 100 * metrics[\"fast_decay_size\"] / total\n",
    "    \n",
    "    if np.any(plateau_mask):\n",
    "        metrics[\"plateau_area\"] = np.sum(log_vals[plateau_mask] - zipf_line[plateau_mask])\n",
    "    else:\n",
    "        metrics[\"plateau_area\"] = 0\n",
    "        \n",
    "    if np.any(fast_decay_mask):\n",
    "        metrics[\"fast_decay_area\"] = np.sum(zipf_line[fast_decay_mask] - log_vals[fast_decay_mask])\n",
    "    else:\n",
    "        metrics[\"fast_decay_area\"] = 0\n",
    "    \n",
    "    if np.sum(plateau_mask) >= 2:\n",
    "        plateau_slope, _, _, _, _ = stats.linregress(\n",
    "            log_ranks[plateau_mask], log_vals[plateau_mask]\n",
    "        )\n",
    "        metrics[\"plateau_slope\"] = plateau_slope\n",
    "    else:\n",
    "        metrics[\"plateau_slope\"] = None\n",
    "        \n",
    "    if np.sum(linear_mask) >= 2:\n",
    "        linear_slope, _, _, _, _ = stats.linregress(\n",
    "            log_ranks[linear_mask], log_vals[linear_mask]\n",
    "        )\n",
    "        metrics[\"linear_slope\"] = linear_slope\n",
    "    else:\n",
    "        metrics[\"linear_slope\"] = None\n",
    "        \n",
    "    if np.sum(fast_decay_mask) >= 2:\n",
    "        fast_decay_slope, _, _, _, _ = stats.linregress(\n",
    "            log_ranks[fast_decay_mask], log_vals[fast_decay_mask]\n",
    "        )\n",
    "        metrics[\"fast_decay_slope\"] = fast_decay_slope\n",
    "    else:\n",
    "        metrics[\"fast_decay_slope\"] = None\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_zipf_with_fixed_transitions(\n",
    "    log_ranks, log_vals, zipf_line, coeffs, \n",
    "    plateau_log_rank=3.5, elbow_log_rank=5.5,\n",
    "    title=\"\", save_path=None, ylim=None\n",
    "):\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 20,\n",
    "        'axes.titlesize': 28,\n",
    "        'axes.labelsize': 25,\n",
    "        'xtick.labelsize': 20,\n",
    "        'ytick.labelsize': 20,\n",
    "        'legend.fontsize': 20\n",
    "    })\n",
    "    sns.set_palette(\"tab10\")\n",
    "    \n",
    "    plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "    \n",
    "    plt.plot(log_ranks, log_vals, '.', alpha=0.6)\n",
    "    \n",
    "    plateau_idx = np.where(log_ranks >= plateau_log_rank)[0][0] if any(log_ranks >= plateau_log_rank) else 0\n",
    "    elbow_idx = np.where(log_ranks >= elbow_log_rank)[0][0] if any(log_ranks >= elbow_log_rank) else len(log_ranks)\n",
    "    \n",
    "    plateau_info = {\n",
    "        \"plateau_detected\": True,\n",
    "        \"index\": plateau_idx,\n",
    "        \"log_rank\": log_ranks[plateau_idx] if plateau_idx < len(log_ranks) else log_ranks[-1],\n",
    "        \"log_value\": log_vals[plateau_idx] if plateau_idx < len(log_vals) else log_vals[-1]\n",
    "    }\n",
    "    \n",
    "    elbow_info = {\n",
    "        \"elbow_detected\": True,\n",
    "        \"index\": elbow_idx,\n",
    "        \"log_rank\": log_ranks[elbow_idx] if elbow_idx < len(log_ranks) else log_ranks[-1],\n",
    "        \"log_value\": log_vals[elbow_idx] if elbow_idx < len(log_vals) else log_vals[-1]\n",
    "    }\n",
    "    \n",
    "    metrics = calculate_phase_metrics(log_ranks, log_vals, zipf_line, plateau_log_rank, elbow_log_rank)\n",
    "    \n",
    "    print(f\"Plateau phase neurons: {metrics['plateau_size']} ({metrics['plateau_percent']:.2f}%)\")\n",
    "    print(f\"Linear phase neurons: {metrics['linear_size']} ({metrics['linear_percent']:.2f}%)\")\n",
    "    print(f\"Fast decay phase neurons: {metrics['fast_decay_size']} ({metrics['fast_decay_percent']:.2f}%)\")\n",
    "    \n",
    "    plt.axvspan(min(log_ranks), plateau_info[\"log_rank\"], alpha=0.1, color='blue')\n",
    "    plt.axvspan(plateau_info[\"log_rank\"], elbow_info[\"log_rank\"], alpha=0.1, color='green')\n",
    "    plt.axvspan(elbow_info[\"log_rank\"], max(log_ranks), alpha=0.1, color='red')\n",
    "    \n",
    "    bottom_y = min(log_vals) - (max(log_vals) - min(log_vals)) * 0.05\n",
    "    \n",
    "    plt.text(min(log_ranks) + (plateau_info[\"log_rank\"] - min(log_ranks))/2, \n",
    "             bottom_y, \n",
    "             f\"{metrics['plateau_percent']:.1f}%\", \n",
    "             ha='center', va='top', color='blue', fontweight='bold')\n",
    "    \n",
    "    plt.text(plateau_info[\"log_rank\"] + (elbow_info[\"log_rank\"] - plateau_info[\"log_rank\"])/2, \n",
    "             bottom_y, \n",
    "             f\"{metrics['linear_percent']:.1f}%\", \n",
    "             ha='center', va='top', color='green', fontweight='bold')\n",
    "    \n",
    "    plt.text(elbow_info[\"log_rank\"] + (max(log_ranks) - elbow_info[\"log_rank\"])/2, \n",
    "             bottom_y, \n",
    "             f\"{metrics['fast_decay_percent']:.1f}%\", \n",
    "             ha='center', va='top', color='red', fontweight='bold')\n",
    "    \n",
    "    plt.scatter(plateau_info[\"log_rank\"], plateau_info[\"log_value\"], \n",
    "                c='blue', marker='D', s=100)\n",
    "    plt.scatter(elbow_info[\"log_rank\"], elbow_info[\"log_value\"], \n",
    "                c='green', marker='X', s=100)\n",
    "    \n",
    "    plt.hlines(plateau_info[\"log_value\"], \n",
    "               xmin=min(log_ranks), xmax=plateau_info[\"log_rank\"],\n",
    "               colors='blue', linestyles='--', linewidth=3.5)\n",
    "    plt.hlines(elbow_info[\"log_value\"], \n",
    "               xmin=min(log_ranks), xmax=elbow_info[\"log_rank\"],\n",
    "               colors='green', linestyles='--', linewidth=3.5)\n",
    "    \n",
    "    plt.xlabel('Log Rank')\n",
    "    plt.ylabel('Log Value')\n",
    "    \n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def simplified_analysis(\n",
    "    root_dir, \n",
    "    output_dir,\n",
    "    target_step=\"114000\",\n",
    "    plateau_log_rank=3.5,\n",
    "    elbow_log_rank=5.5,\n",
    "    ylim=(-12, -3)\n",
    "):\n",
    "    root_path = Path(root_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    csv_files = list(root_path.rglob(\"500_all.csv\"))\n",
    "    print(f\"Found {len(csv_files)} CSV files\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df[\"abs_delta_loss_post_ablation\"] = df[\"abs_delta_loss_post_ablation\"].apply(ast.literal_eval)\n",
    "        df[\"step\"] = df[\"step\"].astype(str)\n",
    "        \n",
    "        rel_id = str(csv_file.relative_to(root_path).parent).replace(\"/\", \"_\")\n",
    "        \n",
    "        step_data = df[df[\"step\"] == target_step]\n",
    "        \n",
    "        if len(step_data) == 0:\n",
    "            print(f\"Warning: Step {target_step} not found in {rel_id}\")\n",
    "            continue\n",
    "        \n",
    "        for _, row in step_data.iterrows():\n",
    "            vals = row[\"abs_delta_loss_post_ablation\"]\n",
    "            \n",
    "            arr = np.array(vals)\n",
    "            arr = arr[arr > 0]\n",
    "            sorted_vals = np.sort(arr)[::-1]\n",
    "            ranks = np.arange(1, len(sorted_vals)+1)\n",
    "            log_ranks = np.log(ranks)\n",
    "            log_vals = np.log(sorted_vals)\n",
    "            \n",
    "            coeffs = np.polyfit(log_ranks, log_vals, deg=1)\n",
    "            zipf_line = np.poly1d(coeffs)(log_ranks)\n",
    "            \n",
    "            title = f\"{rel_id}\"\n",
    "            save_path = output_path / f\"{rel_id}_step{target_step}_zipf_transitions.png\"\n",
    "            \n",
    "            plot_zipf_with_fixed_transitions(\n",
    "                log_ranks, \n",
    "                log_vals, \n",
    "                zipf_line, \n",
    "                coeffs, \n",
    "                plateau_log_rank=plateau_log_rank,\n",
    "                elbow_log_rank=elbow_log_rank,\n",
    "                title=title,\n",
    "                save_path=save_path,\n",
    "                ylim=ylim\n",
    "            )\n",
    "            \n",
    "            metrics = calculate_phase_metrics(\n",
    "                log_ranks, \n",
    "                log_vals, \n",
    "                zipf_line, \n",
    "                plateau_log_rank, \n",
    "                elbow_log_rank\n",
    "            )\n",
    "            \n",
    "            results[rel_id] = {\n",
    "                \"zipf_slope\": coeffs[0],\n",
    "                \"plateau_percent\": metrics[\"plateau_percent\"],\n",
    "                \"linear_percent\": metrics[\"linear_percent\"],\n",
    "                \"fast_decay_percent\": metrics[\"fast_decay_percent\"]\n",
    "            }\n",
    "    \n",
    "    summary_path = output_path / \"phase_summary.json\"\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = simplified_analysis(\n",
    "        root_dir=\"/Users/curiostudio/Desktop/apd/gpt2\",\n",
    "        output_dir=\"/Users/curiostudio/Desktop/apd/\",\n",
    "        target_step=\"last\",\n",
    "        plateau_log_rank=3.5,\n",
    "        elbow_log_rank=5.5,\n",
    "        ylim=(-12, -3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc14937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3d9814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7178b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#线性拟合+截距检测\n",
    "#直接调整Config里的信息就行\n",
    "\n",
    "# Configuration Parameters - Easy to modify in one place\n",
    "class LogRankConfig:\n",
    "    # Log rank range for linear regression\n",
    "    LOG_RANK_MIN = 4.8  # Minimum log rank value for target range\n",
    "    LOG_RANK_MAX = 5.2  # Maximum log rank value for target range\n",
    "    \n",
    "    # Output settings\n",
    "    SAVE_INDIVIDUAL_RESULTS = True   # Whether to save individual file results\n",
    "    SAVE_CONSOLIDATED_RESULTS = True # Whether to save consolidated results\n",
    "    \n",
    "    # Directories\n",
    "    BASE_DIRECTORY=\"/Users/curiostudio/Desktop/apd/gpt2/prob/suppress\"\n",
    "    OUTPUT_DIRECTORY=\"/Users/curiostudio/Desktop/apd/gpt2-bias/50bias_check-suppress48\"\n",
    "\n",
    "\n",
    "\n",
    "# Function to perform linear regression using log rank data within specified range\n",
    "def log_rank_linear_regression(values, config=LogRankConfig):\n",
    "    \"\"\"\n",
    "    Perform linear regression on log rank data within specified range and calculate\n",
    "    the delta bias for each point before the target range.\n",
    "    \n",
    "    Args:\n",
    "        values: List of data values to analyze\n",
    "        config: Configuration parameters\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing regression results and delta biases\n",
    "    \"\"\"\n",
    "    # Remove zeros and sort values in descending order\n",
    "    arr = np.array(values)\n",
    "    arr = arr[arr > 0]  # Remove zeros or negative values\n",
    "    arr = -np.sort(-arr)  # Sort in descending order\n",
    "    \n",
    "    # Compute ranks and convert to log scale\n",
    "    ranks = np.arange(1, len(arr) + 1)\n",
    "    log_ranks = np.log(ranks)  # Using natural log\n",
    "    log_vals = np.log(arr)\n",
    "    \n",
    "    # Extract data points with log rank within the specified range\n",
    "    target_mask = (log_ranks >= config.LOG_RANK_MIN) & (log_ranks <= config.LOG_RANK_MAX)\n",
    "    target_log_ranks = log_ranks[target_mask]\n",
    "    target_log_vals = log_vals[target_mask]\n",
    "    \n",
    "    # If there are not enough data points in the target range, return early\n",
    "    if len(target_log_ranks) < 2:\n",
    "        return {\n",
    "            \"slope\": None,\n",
    "            \"initial_bias\": None,\n",
    "            \"delta_biases\": [],\n",
    "            \"target_range\": {\n",
    "                \"log_ranks\": np.array([]),\n",
    "                \"log_vals\": np.array([]),\n",
    "                \"fitted_vals\": np.array([])\n",
    "            },\n",
    "            \"extended_fit\": {\n",
    "                \"log_ranks\": np.array([]),\n",
    "                \"fitted_vals\": np.array([])\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Perform linear regression on the target range\n",
    "    coeffs = np.polyfit(target_log_ranks, target_log_vals, 1)\n",
    "    slope, initial_bias = coeffs\n",
    "    \n",
    "    # Calculate the delta bias for each point (all points, not just early ones)\n",
    "    # The delta bias is the vertical distance from each point to the regression line\n",
    "    regression_line = lambda x: slope * x + initial_bias\n",
    "    predicted_vals = regression_line(log_ranks)\n",
    "    delta_biases = log_vals - predicted_vals\n",
    "    \n",
    "    # Prepare delta biases with their corresponding ranks for all points\n",
    "    delta_bias_data = []\n",
    "    for i in range(len(log_ranks)):\n",
    "        delta_bias_data.append({\n",
    "            \"rank\": int(ranks[i]),\n",
    "            \"log_rank\": float(log_ranks[i]),\n",
    "            \"log_val\": float(log_vals[i]),\n",
    "            \"predicted_val\": float(predicted_vals[i]),\n",
    "            \"delta_bias\": float(delta_biases[i])\n",
    "        })\n",
    "    \n",
    "    # Generate fitted lines for visualization\n",
    "    target_fitted = slope * target_log_ranks + initial_bias\n",
    "    \n",
    "    # Create extended fit line for full visualization\n",
    "    all_log_ranks = np.linspace(np.min(log_ranks), np.max(log_ranks), 100)\n",
    "    extended_fit = slope * all_log_ranks + initial_bias\n",
    "    \n",
    "    return {\n",
    "        \"slope\": slope,\n",
    "        \"initial_bias\": initial_bias,\n",
    "        \"delta_biases\": delta_bias_data,\n",
    "        \"target_range\": {\n",
    "            \"log_ranks\": target_log_ranks,\n",
    "            \"log_vals\": target_log_vals,\n",
    "            \"fitted_vals\": target_fitted\n",
    "        },\n",
    "        \"extended_fit\": {\n",
    "            \"log_ranks\": all_log_ranks,\n",
    "            \"fitted_vals\": extended_fit\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Function to visualize the results with emphasis on delta biases\n",
    "def plot_log_rank_regression(log_ranks, log_vals, regression_results, title, save_path=None, config=LogRankConfig):\n",
    "    \"\"\"Plot the log rank regression results with focus on delta biases.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12), gridspec_kw={'height_ratios': [2, 1]})\n",
    "    \n",
    "    # Plot 1: Data points and regression fit\n",
    "    ax1.scatter(log_ranks, log_vals, alpha=0.6, s=10, color='blue', label='Data Points')\n",
    "    \n",
    "    # Check if we have valid results (slope is not None)\n",
    "    if regression_results[\"slope\"] is None:\n",
    "        ax1.set_title(f\"{title}\\nInsufficient data in {config.LOG_RANK_MIN}-{config.LOG_RANK_MAX} range\", fontsize=14)\n",
    "        ax1.set_xlabel(\"Log Rank\", fontsize=12)\n",
    "        ax1.set_ylabel(\"Log Value\", fontsize=12)\n",
    "        ax1.legend(loc='best', fontsize=10)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Set up the second subplot (empty)\n",
    "        ax2.set_xlabel(\"Log Rank\", fontsize=12)\n",
    "        ax2.set_ylabel('Delta Bias', fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.text(0.5, 0.5, \"No delta bias data available\", \n",
    "                ha='center', va='center', transform=ax2.transAxes)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300)\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    # Highlight the target range\n",
    "    target_range = regression_results[\"target_range\"]\n",
    "    if len(target_range[\"log_ranks\"]) > 0:\n",
    "        ax1.scatter(target_range[\"log_ranks\"], target_range[\"log_vals\"], \n",
    "                   color='red', s=40, label=f'Target Range ({config.LOG_RANK_MIN}-{config.LOG_RANK_MAX})')\n",
    "    \n",
    "    # Plot the extended fit line across the entire range\n",
    "    if len(regression_results[\"extended_fit\"][\"log_ranks\"]) > 0:\n",
    "        ax1.plot(regression_results[\"extended_fit\"][\"log_ranks\"], \n",
    "                regression_results[\"extended_fit\"][\"fitted_vals\"], \n",
    "                linestyle='-', linewidth=2.5, color='green',\n",
    "                label=f'Linear Fit (Slope={regression_results[\"slope\"]:.3f}, Bias={regression_results[\"initial_bias\"]:.3f})')\n",
    "    \n",
    "    # Shade the target region\n",
    "    y_min, y_max = ax1.get_ylim()\n",
    "    ax1.fill_between([config.LOG_RANK_MIN, config.LOG_RANK_MAX], y_min, y_max, color='yellow', alpha=0.1)\n",
    "    \n",
    "    # Add vertical lines at log rank boundaries\n",
    "    ax1.axvline(x=config.LOG_RANK_MIN, color='gray', linestyle='--', alpha=0.7)\n",
    "    ax1.axvline(x=config.LOG_RANK_MAX, color='gray', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    ax1.set_title(title, fontsize=14)\n",
    "    ax1.set_xlabel(\"Log Rank\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Log Value\", fontsize=12)\n",
    "    ax1.legend(loc='best', fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Delta bias values\n",
    "    if regression_results[\"delta_biases\"]:\n",
    "        # Extract delta bias values and their corresponding log ranks\n",
    "        # Filter to only show points before the target range\n",
    "        early_deltas = [db for db in regression_results[\"delta_biases\"] if db[\"log_rank\"] < config.LOG_RANK_MIN]\n",
    "        \n",
    "        if early_deltas:\n",
    "            log_rank_positions = [db[\"log_rank\"] for db in early_deltas]\n",
    "            delta_bias_values = [db[\"delta_bias\"] for db in early_deltas]\n",
    "            \n",
    "            # Plot delta bias values against log rank\n",
    "            ax2.plot(log_rank_positions, delta_bias_values, 'r-o', linewidth=2, markersize=4)\n",
    "            ax2.set_xlabel(\"Log Rank\", fontsize=12)\n",
    "            ax2.set_ylabel('Delta Bias', fontsize=12)\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add horizontal line at delta bias = 0\n",
    "            ax2.axhline(y=0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
    "            \n",
    "            # Color positive and negative regions\n",
    "            ax2.fill_between(log_rank_positions, 0, delta_bias_values, \n",
    "                            where=(np.array(delta_bias_values) > 0), \n",
    "                            color='green', alpha=0.1)\n",
    "            ax2.fill_between(log_rank_positions, delta_bias_values, 0, \n",
    "                            where=(np.array(delta_bias_values) < 0), \n",
    "                            color='red', alpha=0.1)\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, \"No early delta bias data available\", \n",
    "                    ha='center', va='center', transform=ax2.transAxes)\n",
    "    else:\n",
    "        ax2.set_xlabel(\"Log Rank\", fontsize=12)\n",
    "        ax2.set_ylabel('Delta Bias', fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.text(0.5, 0.5, \"No delta bias data available\", \n",
    "                ha='center', va='center', transform=ax2.transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Function to plot aggregated delta bias dynamics across all steps (FIXED VERSION)\n",
    "def plot_delta_bias_dynamics(all_delta_biases, output_path, file_id, config=LogRankConfig):\n",
    "    \"\"\"\n",
    "    Plot the sum of delta biases for each log rank across all steps.\n",
    "    \n",
    "    Args:\n",
    "        all_delta_biases: Dictionary mapping step to delta bias data\n",
    "        output_path: Path to save the plot\n",
    "        file_id: Identifier for the file being processed\n",
    "        config: Configuration parameters\n",
    "    \"\"\"\n",
    "    if not all_delta_biases:\n",
    "        print(f\"No delta bias data available for {file_id}\")\n",
    "        return\n",
    "    \n",
    "    # First, collect all the delta biases by rank\n",
    "    rank_to_delta_biases = {}\n",
    "    \n",
    "    for step, deltas in all_delta_biases.items():\n",
    "        for delta_data in deltas:\n",
    "            # Only consider points before the target range\n",
    "            if delta_data[\"log_rank\"] < config.LOG_RANK_MAX:\n",
    "                rank = delta_data[\"rank\"]\n",
    "                if rank not in rank_to_delta_biases:\n",
    "                    rank_to_delta_biases[rank] = []\n",
    "                rank_to_delta_biases[rank].append(delta_data[\"delta_bias\"])\n",
    "    \n",
    "    # Calculate the sum of delta biases for each rank\n",
    "    rank_to_sum_delta = {}\n",
    "    for rank, biases in rank_to_delta_biases.items():\n",
    "        rank_to_sum_delta[rank] = sum(biases)\n",
    "    \n",
    "    # Sort by rank for plotting\n",
    "    sorted_ranks = sorted(rank_to_sum_delta.keys())\n",
    "    log_ranks = [np.log(r) for r in sorted_ranks]\n",
    "    sum_delta_biases = [rank_to_sum_delta[r] for r in sorted_ranks]\n",
    "    \n",
    "    # Create the plot for sum of delta biases\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Check if we have data to plot\n",
    "    if not log_ranks or not sum_delta_biases:\n",
    "        plt.text(0.5, 0.5, \"No delta bias data available for plotting\", \n",
    "                ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.savefig(output_path, dpi=300)\n",
    "        plt.close()\n",
    "        return\n",
    "    \n",
    "    plt.plot(log_ranks, sum_delta_biases, 'b-o', linewidth=2, markersize=6)\n",
    "    \n",
    "    # Add a zero line\n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    # Color positive and negative regions\n",
    "    plt.fill_between(log_ranks, 0, sum_delta_biases, \n",
    "                    where=(np.array(sum_delta_biases) > 0), \n",
    "                    color='green', alpha=0.2)\n",
    "    plt.fill_between(log_ranks, sum_delta_biases, 0, \n",
    "                    where=(np.array(sum_delta_biases) < 0), \n",
    "                    color='red', alpha=0.2)\n",
    "    \n",
    "    plt.title(f\"{file_id} - Sum of Delta Biases Across All Steps\", fontsize=14)\n",
    "    plt.xlabel(\"Log Rank\", fontsize=12)\n",
    "    plt.ylabel(\"Sum of Delta Biases\", fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add markers for key positions\n",
    "    plt.axvline(x=config.LOG_RANK_MIN, color='red', linestyle='--', alpha=0.7, \n",
    "               label=f'Target Range Start ({config.LOG_RANK_MIN})')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Also calculate and plot the average delta bias for each rank\n",
    "    rank_to_avg_delta = {}\n",
    "    for rank, biases in rank_to_delta_biases.items():\n",
    "        rank_to_avg_delta[rank] = sum(biases) / len(biases)\n",
    "    \n",
    "    # Get the sorted values for average plot\n",
    "    avg_delta_biases = [rank_to_avg_delta[r] for r in sorted_ranks]\n",
    "    \n",
    "    # Create the plot for average delta biases\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(log_ranks, avg_delta_biases, 'g-o', linewidth=2, markersize=6)\n",
    "    \n",
    "    # Add a zero line\n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
    "    \n",
    "    # Color positive and negative regions\n",
    "    plt.fill_between(log_ranks, 0, avg_delta_biases, \n",
    "                    where=(np.array(avg_delta_biases) > 0), \n",
    "                    color='green', alpha=0.2)\n",
    "    plt.fill_between(log_ranks, avg_delta_biases, 0, \n",
    "                    where=(np.array(avg_delta_biases) < 0), \n",
    "                    color='red', alpha=0.2)\n",
    "    \n",
    "    plt.title(f\"{file_id} - Average Delta Bias Across All Steps\", fontsize=14)\n",
    "    plt.xlabel(\"Log Rank\", fontsize=12)\n",
    "    plt.ylabel(\"Average Delta Bias\", fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add markers for key positions\n",
    "    plt.axvline(x=config.LOG_RANK_MIN, color='red', linestyle='--', alpha=0.7,\n",
    "               label=f'Target Range Start ({config.LOG_RANK_MIN})')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.tight_layout()\n",
    "    avg_output_path = str(output_path).replace('.png', '_avg.png')\n",
    "    plt.savefig(avg_output_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Function to process a dataset with the new approach\n",
    "def process_dataset_log_rank(data_path, output_dir, config=LogRankConfig):\n",
    "    \"\"\"\n",
    "    Process a dataset using the log rank regression approach.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the CSV data file\n",
    "        output_dir: Directory to save output visualizations\n",
    "        config: Configuration parameters\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get the file ID for naming output files\n",
    "    file_id = Path(data_path).stem\n",
    "    \n",
    "    # Create a folder for this specific CSV file's results\n",
    "    file_output_dir = output_path / file_id\n",
    "    file_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    results = []\n",
    "    all_delta_biases = {}  # Store delta biases by step\n",
    "    all_data_for_csv = []  # Store all data for CSV export\n",
    "    \n",
    "    try:\n",
    "        # Load and process data\n",
    "        df = pd.read_csv(data_path)\n",
    "        \n",
    "        # If your data is stored as string representation of lists, convert it\n",
    "        if isinstance(df[\"abs_delta_loss_post_ablation\"].iloc[0], str):\n",
    "            df[\"abs_delta_loss_post_ablation\"] = df[\"abs_delta_loss_post_ablation\"].apply(ast.literal_eval)\n",
    "        \n",
    "        # Process each row (e.g., each training step)\n",
    "        for idx, row in df.iterrows():\n",
    "            step = row.get(\"step\", f\"row_{idx}\")\n",
    "            vals = row[\"abs_delta_loss_post_ablation\"]\n",
    "            \n",
    "            # Skip if vals is empty or all zeros\n",
    "            if not vals or all(v == 0 for v in vals):\n",
    "                continue\n",
    "            \n",
    "            # Remove zeros and sort values in descending order\n",
    "            arr = np.array(vals)\n",
    "            arr = arr[arr > 0]\n",
    "            arr = -np.sort(-arr)\n",
    "            \n",
    "            # Compute ranks and log values\n",
    "            ranks = np.arange(1, len(arr) + 1)\n",
    "            log_ranks = np.log(ranks)\n",
    "            log_vals = np.log(arr)\n",
    "            \n",
    "            # Perform log rank regression\n",
    "            regression_results = log_rank_linear_regression(vals, config)\n",
    "            \n",
    "            # Store delta biases for this step\n",
    "            if regression_results[\"slope\"] is not None:\n",
    "                # Only store delta biases for points before the target range\n",
    "                early_deltas = [db for db in regression_results[\"delta_biases\"] \n",
    "                               if db[\"log_rank\"] < config.LOG_RANK_MIN]\n",
    "                if early_deltas:\n",
    "                    all_delta_biases[step] = early_deltas\n",
    "                \n",
    "                # Add data to CSV collection\n",
    "                for delta_data in regression_results[\"delta_biases\"]:\n",
    "                    all_data_for_csv.append({\n",
    "                        \"file\": file_id,\n",
    "                        \"step\": step,\n",
    "                        \"rank\": delta_data[\"rank\"],\n",
    "                        \"log_rank\": delta_data[\"log_rank\"],\n",
    "                        \"log_val\": delta_data[\"log_val\"],\n",
    "                        \"predicted_val\": delta_data[\"predicted_val\"],\n",
    "                        \"delta_bias\": delta_data[\"delta_bias\"]\n",
    "                    })\n",
    "            \n",
    "            # Generate output file name\n",
    "            save_path = file_output_dir / f\"step{step}_log_rank_regression.png\"\n",
    "            \n",
    "            # Plot and save results\n",
    "            title = f\"{file_id} - Step {step} Log Rank Regression\"\n",
    "            fig = plot_log_rank_regression(log_ranks, log_vals, regression_results, title, save_path, config)\n",
    "            \n",
    "            # Save regression results if we have valid slope\n",
    "            if regression_results[\"slope\"] is not None:\n",
    "                step_results = {\n",
    "                    \"file\": file_id,\n",
    "                    \"step\": step,\n",
    "                    \"slope\": regression_results[\"slope\"],\n",
    "                    \"initial_bias\": regression_results[\"initial_bias\"],\n",
    "                    \"delta_biases\": regression_results[\"delta_biases\"],\n",
    "                }\n",
    "                results.append(step_results)\n",
    "            \n",
    "            # Close figure to free memory\n",
    "            plt.close(fig)\n",
    "        \n",
    "        # Generate and save the delta bias dynamics plot\n",
    "        if all_delta_biases:\n",
    "            print(f\"Creating delta bias dynamics plot for {file_id} with {len(all_delta_biases)} steps\")\n",
    "            dynamics_path = file_output_dir / f\"{file_id}_delta_bias_dynamics.png\"\n",
    "            plot_delta_bias_dynamics(all_delta_biases, dynamics_path, file_id, config)\n",
    "        else:\n",
    "            print(f\"No valid delta bias data for {file_id}\")\n",
    "        \n",
    "        # Save all data to CSV\n",
    "        if all_data_for_csv:\n",
    "            data_df = pd.DataFrame(all_data_for_csv)\n",
    "            data_path = file_output_dir / f\"{file_id}_delta_bias_data.csv\"\n",
    "            data_df.to_csv(data_path, index=False)\n",
    "            print(f\"Delta bias data saved to {data_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {data_path}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return results, all_delta_biases\n",
    "\n",
    "# Main function to run the analysis\n",
    "def run_log_rank_analysis(config=LogRankConfig):\n",
    "    \"\"\"\n",
    "    Run log rank regression analysis on all CSV files in the base directory.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration parameters\n",
    "    \"\"\"\n",
    "    # Create the output directory\n",
    "    output_path = Path(config.OUTPUT_DIRECTORY)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save configuration for reference\n",
    "    config_summary = {\n",
    "        \"log_rank_min\": config.LOG_RANK_MIN,\n",
    "        \"log_rank_max\": config.LOG_RANK_MAX,\n",
    "        \"save_individual_results\": config.SAVE_INDIVIDUAL_RESULTS,\n",
    "        \"save_consolidated_results\": config.SAVE_CONSOLIDATED_RESULTS,\n",
    "        \"timestamp\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    # Save configuration to JSON\n",
    "    import json\n",
    "    with open(output_path / \"analysis_config.json\", 'w') as f:\n",
    "        json.dump(config_summary, f, indent=2)\n",
    "    \n",
    "    # Find all CSV files in the base directory\n",
    "    csv_files = list(Path(config.BASE_DIRECTORY).glob(\"**/*.csv\"))\n",
    "    \n",
    "    all_results = []\n",
    "    all_delta_biases_by_file = {}\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        print(f\"Processing {csv_file}\")\n",
    "        results, file_delta_biases = process_dataset_log_rank(csv_file, config.OUTPUT_DIRECTORY, config)\n",
    "        all_results.extend(results)\n",
    "        all_delta_biases_by_file[csv_file.stem] = file_delta_biases\n",
    "    \n",
    "    # Save consolidated results if enabled and there are results\n",
    "    if config.SAVE_CONSOLIDATED_RESULTS and all_results:\n",
    "        # Create a consolidated CSV for all delta bias data\n",
    "        all_data = []\n",
    "        for file_id, step_deltas in all_delta_biases_by_file.items():\n",
    "            for step, deltas in step_deltas.items():\n",
    "                for delta_data in deltas:\n",
    "                    all_data.append({\n",
    "                        \"file\": file_id,\n",
    "                        \"step\": step,\n",
    "                        \"rank\": delta_data[\"rank\"],\n",
    "                        \"log_rank\": delta_data[\"log_rank\"],\n",
    "                        \"log_val\": delta_data[\"log_val\"],\n",
    "                        \"predicted_val\": delta_data[\"predicted_val\"],\n",
    "                        \"delta_bias\": delta_data[\"delta_bias\"]\n",
    "                    })\n",
    "        \n",
    "        # Save consolidated delta bias data\n",
    "        if all_data:\n",
    "            consolidated_df = pd.DataFrame(all_data)\n",
    "            results_path = output_path / \"consolidated_delta_bias_data.csv\"\n",
    "            consolidated_df.to_csv(results_path, index=False)\n",
    "            print(f\"Consolidated delta bias data saved to {results_path}\")\n",
    "    \n",
    "    print(f\"Processed {len(csv_files)} datasets\")\n",
    "    return all_results, all_delta_biases_by_file\n",
    "\n",
    "# Example of how to run with customized configuration\n",
    "def main():\n",
    "    # Default configuration\n",
    "    config = LogRankConfig()\n",
    "    \n",
    "    # Uncomment and modify these lines to customize the configuration\n",
    "    # config.LOG_RANK_MIN = 3.5\n",
    "    # config.LOG_RANK_MAX = 4.0\n",
    "    \n",
    "    # Run the analysis\n",
    "    results, all_delta_biases = run_log_rank_analysis(config)\n",
    "    \n",
    "    return results, all_delta_biases\n",
    "\n",
    "# Run the analysis\n",
    "results, all_delta_biases = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6633a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97bd35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc8953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88263cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc8b94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#或许会用到的loss密度分布图\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import ast\n",
    "\n",
    "def run_kde_analysis(root_dir: str, output_dir: str=None):\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 20,\n",
    "        'axes.titlesize': 28,\n",
    "        'axes.labelsize': 25,\n",
    "        'xtick.labelsize': 20,\n",
    "        'ytick.labelsize': 20,\n",
    "        'legend.fontsize': 20\n",
    "    })\n",
    "    \n",
    "    sns.set_palette(\"tab10\")\n",
    "    \n",
    "    #selected_steps = [\"0\", \"14000\", \"34000\", \"54000\", \"64000\", \"84000\", \"104000\", \"114000\", \"124000\"]\n",
    "    selected_steps = [\"last\"]\n",
    "    \n",
    "    root_path = Path(root_dir)\n",
    "    if output_dir:\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    csv_files = list(root_path.rglob(\"500_all.csv\"))\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df[\"abs_delta_loss_post_ablation\"] = df[\"abs_delta_loss_post_ablation\"].apply(ast.literal_eval)\n",
    "        df[\"step\"] = df[\"step\"].astype(str)\n",
    "        \n",
    "        df = df[df['step'].isin(selected_steps)]\n",
    "        \n",
    "        rel_id = str(csv_file.relative_to(root_path).parent).replace(\"/\", \"_\")\n",
    "        \n",
    "        plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "        for _, row in df.iterrows():\n",
    "            vals = row[\"abs_delta_loss_post_ablation\"]\n",
    "            arr = np.array(vals)\n",
    "            arr = arr[arr > 0]\n",
    "            if len(arr) < 5:\n",
    "                continue\n",
    "            sns.kdeplot(arr, label=f\"{row['step']}\", linewidth=3.5)\n",
    "        \n",
    "        plt.xlabel(\"ΔLoss\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        \n",
    "        #这里调节x轴段范围\n",
    "        #plt.xlim(0, 0.0007)\n",
    "        \n",
    "        plt.legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if output_dir:\n",
    "            save_kde = output_path / f\"{rel_id}_selected_steps_kde.png\"\n",
    "            plt.savefig(save_kde,dpi=200)\n",
    "        \n",
    "        results[rel_id] = plt.gcf()\n",
    "        plt.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# root_dir = \"your_data_directory\"\n",
    "# output_dir = \"output_directory\"\n",
    "# kde_plots = run_kde_analysis(root_dir)\n",
    "# list(kde_plots.keys())\n",
    "# kde_plots['your_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754f7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39739be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f54f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d9c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#以此往下是一阶导折点图，改最下面的参数（这是原始版本，我给文章发图的版本好像没保存或是按错删了不见了…不过只用改下尺寸就行没关系，后面我加的都是二阶导的）\n",
    "# check derivatives by steps\n",
    "# get rank and values\n",
    "\n",
    "def rank_val(values)->tuple[np.array,np.array]:\n",
    "    \"\"\"Analyze the distribution and identify both transition points using a unified approach. \"\"\"\n",
    "    arr = np.array(values)\n",
    "    arr = arr[arr > 0]\n",
    "    sorted_vals = np.sort(arr)[::-1]\n",
    "    ranks = np.arange(1, len(sorted_vals)+1)\n",
    "    return np.log(ranks), np.log(sorted_vals)\n",
    "\n",
    "\n",
    "def plot_slope(\n",
    "    log_ranks: np.ndarray,\n",
    "    log_vals: np.ndarray,\n",
    "    window_size: int = 50,\n",
    ") -> dict:\n",
    "    \"\"\"Get first derivatives of each step.\"\"\"\n",
    "    n = len(log_ranks)\n",
    "    \n",
    "    # Apply rank percentile constraints\n",
    "    max_idx = min(int(len(log_ranks)), n - window_size)\n",
    "    # Dynamic window size\n",
    "    w = min(window_size, max(5, (max_idx - window_size) // 5))\n",
    "    # Calculate first derivatives (slopes)\n",
    "    derivatives, idxs = [], []\n",
    "    for i in range(window_size, max_idx - w + 1):\n",
    "        slope, _, _, _, _ = stats.linregress(\n",
    "            log_ranks[i : i + w], log_vals[i : i + w]\n",
    "        )\n",
    "        derivatives.append(slope)\n",
    "        idxs.append(i)\n",
    "    derivatives = np.array(derivatives)\n",
    "    return derivatives\n",
    "\n",
    "\n",
    "def run_analysis(\n",
    "    root_dir: str, \n",
    "    output_dir: str, \n",
    "    ylim: tuple = None,\n",
    "    window_size: int = 50,\n",
    "):\n",
    "    \"\"\"Batch process all 500_all.csv files in the directory,\"\"\"\n",
    "    root_path = Path(root_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    # Find all CSV files\n",
    "    csv_files = list(root_path.rglob(\"500_all.csv\"))\n",
    "    \n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        # Read and preprocess the CSV\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df[\"abs_delta_loss_post_ablation\"] = df[\"abs_delta_loss_post_ablation\"].apply(ast.literal_eval)\n",
    "        df[\"step\"] = df[\"step\"].astype(str)\n",
    "        rel_id = str(csv_file.relative_to(root_path).parent).replace(\"/\", \"_\")\n",
    "        \n",
    "        \n",
    "        # Analyze each training step\n",
    "        for _, row in df.iterrows():\n",
    "            vals = row[\"abs_delta_loss_post_ablation\"]\n",
    "            log_ranks, log_vals = rank_val(vals)\n",
    "            derivatives = plot_slope(log_ranks, log_vals)\n",
    "            plt.plot(log_ranks[:-window_size],derivatives)\n",
    "            plt.show()\n",
    "            \n",
    "            # Apply the unified approach with custom parameters\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f904e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_slope(\n",
    "    log_ranks: np.ndarray,\n",
    "    log_vals: np.ndarray,\n",
    "    window_size: int = 2,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Get first derivatives of each step.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (derivatives, indices) where derivatives are the slopes and \n",
    "               indices are the corresponding positions in the original array\n",
    "    \"\"\"\n",
    "    n = len(log_ranks)\n",
    "    # Apply rank percentile constraints\n",
    "    max_idx = min(int(len(log_ranks)), n - window_size)\n",
    "    # Dynamic window size\n",
    "    w = min(window_size, max(5, (max_idx - window_size) // 5))\n",
    "\n",
    "    # Calculate first derivatives (slopes)\n",
    "    derivatives, idxs = [], []\n",
    "    for i in range(window_size, max_idx - w + 1):\n",
    "        slope, _, _, _, _ = stats.linregress(\n",
    "            log_ranks[i : i + w*i], log_vals[i : i + w*i]\n",
    "        )\n",
    "        derivatives.append(slope)\n",
    "        idxs.append(i)\n",
    "    return np.array(derivatives), np.array(idxs)\n",
    "\n",
    "def run_analysis(\n",
    "    root_dir: str,\n",
    "    output_dir: str,\n",
    "    ylim: tuple = None,\n",
    "    window_size = 5,\n",
    "    step_lst = [],\n",
    "):\n",
    "    \"\"\"Batch process all 500_all.csv files in the directory.\"\"\"\n",
    "    root_path = Path(root_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    # Find all CSV files\n",
    "    csv_files = list(root_path.rglob(\"500_all.csv\"))\n",
    "    for csv_file in csv_files:\n",
    "        # Read and preprocess the CSV\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df[\"abs_delta_loss_post_ablation\"] = df[\"abs_delta_loss_post_ablation\"].apply(ast.literal_eval)\n",
    "        df[\"step\"] = df[\"step\"].astype(str)\n",
    "        rel_id = str(csv_file.relative_to(root_path).parent).replace(\"/\", \"_\")\n",
    "        \n",
    "        # Analyze each training step\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            vals = row[\"abs_delta_loss_post_ablation\"]\n",
    "            step = str(row[\"step\"])\n",
    "            log_ranks, log_vals = rank_val(vals)\n",
    "            if step in step_lst:\n",
    "                derivatives, idxs = plot_slope(log_ranks, log_vals, window_size)\n",
    "                # Use the correct indices for plotting\n",
    "                plt.plot(log_ranks[idxs], derivatives,label = step)\n",
    "            \n",
    "        plt.title(f\"{rel_id}\")\n",
    "        plt.xlabel(\"Log rank\")\n",
    "        plt.ylabel(\"Slope\")\n",
    "        plt.ylim(ylim)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        # Apply the unified approach with custom parameterswindow_size: int = 10,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33206cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_metrics = run_analysis(\n",
    "    root_dir=f\"{ROOT_dir}/results/selection/neuron/longtail_50\",\n",
    "    output_dir=f\"{ROOT_dir}/fig/loss_slope\",\n",
    "    ylim=[-1.5,0],\n",
    "    window_size = 5,\n",
    "    step_lst = [\"4000\",\"24000\", \"64000\",\"143000\"]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
